<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[ObsidianVault]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>ObsidianVault</title><link></link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Mon, 05 Aug 2024 13:35:06 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Mon, 05 Aug 2024 13:34:47 GMT</pubDate><ttl>60</ttl><dc:creator></dc:creator><item><title><![CDATA[Index - AI topics]]></title><description><![CDATA[ 
 <br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>]]></description><link>_indexes\index-ai-topics.html</link><guid isPermaLink="false">_Indexes/Index - AI topics.md</guid><pubDate>Wed, 31 Jul 2024 20:40:59 GMT</pubDate></item><item><title><![CDATA[Index - اعادة تشغيل الذات]]></title><description><![CDATA[ 
 <br><br><br><br>]]></description><link>_indexes\index-اعادة-تشغيل-الذات.html</link><guid isPermaLink="false">_Indexes/Index - اعادة تشغيل الذات.md</guid><pubDate>Wed, 31 Jul 2024 20:42:41 GMT</pubDate></item><item><title><![CDATA[Index - ما لا يسع الملسم جهله]]></title><description><![CDATA[ 
 <br><br><br><br>]]></description><link>_indexes\index-ما-لا-يسع-الملسم-جهله.html</link><guid isPermaLink="false">_Indexes/Index - ما لا يسع الملسم جهله.md</guid><pubDate>Wed, 31 Jul 2024 21:00:57 GMT</pubDate></item><item><title><![CDATA[خريطة الدورة :]]></title><description><![CDATA[<a class="tag" href="?query=tag:انس_السلطان" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#انس_السلطان</a> <a class="tag" href="?query=tag:انس_السلطان" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#انس_السلطان</a> 
 <br><br>(1) مقدمة<br>
(2,3 ) في العقيدة<br>
(4,5,6) في الفقة<br>
(7,8) في التزكية <br><br><br>سؤال : لماذا نتعلم  ؟ <br><br>
<br>لان التعلم و طلب المعرفة غريزة في الانسان ( الفضول ) . 
<br>رغبة التطور من اجل ان تكون حياتنا افضل . 
<br><br>
<br>لان العلم طريق خشية الله " إنما يخشي الله من عباده العلماء " .
<br>الملخص : لان طلب العلم ضرورة بشرية و فريضة شرعية . <br><br>حديث ام السنة : مجيء سيدنا جبريل علي هيئة رجل الي سيدنا محمد صل الله عليه و سلم .<br>
من علامات الساعة : الرجل المناسب في المكان الغير مناسب ، و ان تصبح البنت سيدة امها يعني كثرة عقوق الوالدين .<br>
" العلم صيد و الكتابة قيده ، قيد صيودك بالحبال الواثقة ، قيد صيودك لا تدعها طالقة " - منقول عن <a href=".?query=tag:انس_السلطان" class="tag" target="_blank" rel="noopener">#انس_السلطان</a><br>
" ينبعي ان نتعلم ما يكون به الانسان انسانا ، قبل ان نتعلم ما يكون به المسلم مسلما " - <a href=".?query=tag:انس_السلطان" class="tag" target="_blank" rel="noopener">#انس_السلطان</a>  ( نظرية الدلو ) .<br>
العلم المقصود هو اي علم يوصل الي الله .<br>
عن سيدنا علي ابن طالب قال : " الناس ثلاث نفر : فعال رباني ، و متعلم علي سبيل النجاه ، و همل رعاع اتباع ككل ناعق يميلون حيث مالت الريخ لم يستضيئوا بنور العلم ، و لم يركنوا الي ركن وثيق ، قال رسول الله صل الله عليه و سلم : " كن عالما او متعلما ولا تكن الثالثة فتهلك " " <br><br><br>سؤال : كيف يؤدي الفقة الي عمارة الارض ؟<br>
بشموله : شمول مباحثة لكافة نواحي الحياة<br>
و ينقسم علم الفقه الي 6 فروع : <br>
<br>العبادات ( ايماني ) 
<br>المعاملات ( اقتصادي و اجتماعي ) 
<br>المناكحات و المواريث ( الاحوال الشخصية ) 
<br>الاقضية و الشهادات ( القضائي ) 
<br>الجنايات 
<br>الجهاد 
<br><br>الروح هي السر الالهي المحرك لكل ذلك .<br>
النفس الشخصية الموصوفة بالحسد او القبح تتجه علي ما يصدر عن العقل و الجسد و القلب [فكر - شعور - سلوك ] <br><br><br>
<br>الالهيات 
<br>النبوات 
<br>السمعيات 
<br><br><br>
<br>علاقة الانسان بالله 
<br>علاقة الانسان بنفسه 
<br>علاقة الانسان بالاخرين 
<br><br>قاعدة : " كل ما هو واجب عمله وجب علمه " . ]]></description><link>_zettlenotes\other-notes\01-الدرس-الاول.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/01 الدرس الاول.md</guid><dc:creator><![CDATA[انس السلطان]]></dc:creator><pubDate>Thu, 25 Jul 2024 23:19:53 GMT</pubDate></item><item><title><![CDATA[02 الدرس الثاني]]></title><description><![CDATA[<a class="tag" href="?query=tag:الغزالي" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#الغزالي</a> <a class="tag" href="?query=tag:اسامة_السلطان" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#اسامة_السلطان</a> <a class="tag" href="?query=tag:رسول_الله" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#رسول_الله</a> <a class="tag" href="?query=tag:انس_السلطان" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#انس_السلطان</a> 
 <br>" ليس في الامكان ابدع مما كان " - <a href=".?query=tag:الغزالي" class="tag" target="_blank" rel="noopener">#الغزالي</a><br>
قاعدة : " اعمل لما بقي بقدر ما يبقي " و المعني ان الاشياء الزائلات علي درجات ، و ابني الاساس الاول و بعدين إن فاض وقت فكر في الزينة .<br>
كل الانبياء بعد سيدنا ابراهيم يسمون انبياء اخر الزمان<br>
سيدنا نوح اطول الناس عمراُ عاش 1650 عام منهم 950 فترة الدعوة فقط .<br>
قال الشاعر : " لو يعلم الناس ماذا في بطونهم - ما استشعر العجب شبانا و لا شيب "<br>
قاعدة : " لن يصل الانسان الي تمام المعرفة بالله الا بتمام معرفتة بنفسه و لن تتم معرفته بنفسه الا اذا لم ينظر اليها "<br>
قاعدة : " كل و هم يتبعة هم " - <a href=".?query=tag:اسامة_السلطان" class="tag" target="_blank" rel="noopener">#اسامة_السلطان</a><br>
قال <a href=".?query=tag:رسول_الله" class="tag" target="_blank" rel="noopener">#رسول_الله</a>  صلي الله عليه و سلم : " الناس نيام فإذا ماتوا انتبهوا " <br><br><img style="max-width:400px; " class="excalidraw-svg excalidraw-embedded-img excalidraw-canvas-immersive" src="blob:\\5d2e17bd-aeed-44fb-acc4-06a38702303f" filesource="Excalidraw/Drawing 2024-07-26 02.24.38.excalidraw.md" w="400" draggable="false" oncanvas="false"><br>اسماء الله الحسني للنابلسي ، او ايها الولد للغزالي <br><br><br>
<br>الحكم هو إثبات امر لامر او نفي امر عن امر 
<br><br>
<br>عقلي<br>
يستطيع العقل البشري ايجاده دن تجارب مثال : الاب اكبر من الابن 
<br>عادي<br>
يأتي بالتجارب مثال الفلزات تتمدد بالحرارة و تنكمش بالبروده 
<br>شرعي<br>
يثبت بالشرع ، الله يثبت اشياء و يبقي اشياء 
<br><br>و ال 3 احكام لا يتعارضوا مع بعضهم<br>
تلخيص الاحكام في بيت شعر : " اقسام حكم العقل لامحالة - هو الوجوب ثم الاستحالة ثم الجواز ثالث الاقسام فأفهم منحت لذة الافهام " - منقول <a href=".?query=tag:انس_السلطان" class="tag" target="_blank" rel="noopener">#انس_السلطان</a> ]]></description><link>_zettlenotes\other-notes\02-الدرس-الثاني.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/02 الدرس الثاني.md</guid><dc:creator><![CDATA[انس السلطان]]></dc:creator><pubDate>Thu, 25 Jul 2024 23:34:39 GMT</pubDate><enclosure url="blob:app:\\obsidian.md\5d2e17bd-aeed-44fb-acc4-06a38702303f" length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;blob:app:\\obsidian.md\5d2e17bd-aeed-44fb-acc4-06a38702303f&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[احكام العقل]]></title><description><![CDATA[ 
 <br><br>ما هو الواجب العقلي ؟<br>
هو ما لا يقبل التخلف لذاته ، مثل لا يوجد اثر بلا مؤثر <br><br>
<br>كل كمال يليق بذاته المقدسة 
<br>الصفة الذاتية<br>
انه تعالي موجود<br>
انواع الموجودات : 

<br>واجب الوجود : ما لم يسبق بعدم ، سبب الموجودات ، لان التسلسل باطل 
<br>جائز الوجود : تحتاج الي مُوجد<br>
المعارف :<br>
1. ضروري ، هو ما لا يحتاج الي التأمل " هو الاساسيات العقلية التي نرجع اليها "<br>
2. نظري : هو ما احتاج الي التأمل<br>
1. ما هو التفكير : استخدام المعلومات من اجل الوصول الي المجهولات<br>
2. السفصطه : هي انكار المحسوسات ، و انه كل شيء نسبي<br>
3. مقولة : " و ليس يصح في الاذهان شيء ، اذا احتاج النهار الي دليل " ، يعني انسي النقاش<br>
4. كل متغير حادث ، و الحادث لا يوجد علي القديم<br>
5. الحادث هو غير قديم ، يمعني له بداية ، و القديم ليس له بداية ، القديم ، القديم هو الله و كل ما سواه حادث .<br>
6. القديم لا يتغير يعني لا يتغير في اي شيء ، لذلك كل الكون متغير  


<br>انه قديم اي ليس قبله شيء 
<br>البقاء اي ليس بعده شيء 
<br>المخالفة للحوادث اي انه لا يتغير سبحانه و تعالي و ليس له بداية 
<br>القيام بالنفس اي لا يحتاج الي شيء 
<br>الوحدانية اي ليس معه شيء<br>
الايمان و الاحسان و الاسلام : توازية و ليس متتابعة ( فكر الايمان -  و شعور الاحسان ثم فعل او سلوك الاسلام )
<br><br><br><br>
<br>الجواب لأ ، لان الله تعالي لا يوصف انه في مكان او في زمان لان الزمان و المكان من خلق الله و المخلوق لا يُحيط بالخالق ، و تنزله معناها تنزل رحماته و ليلك انت و ليس ليل الله لان الله ليس عنده صبح و ليل . 
<br>العرش هو خلق من خلق الله عجيب لا نعلم كيف هو ، و ليس شبه الكرسي ، معني قوله تعالي : " الرحمن علي العرش استوي " اي حكم و ضبط و ليس جلس فإذا الله تعالي ضبط اكبر مخلوق و هو العرض اليس بقادر  ان يضبط ما دونه . 
<br><br>
<br>الجواب لا غير صحيح ، فيه حاجه اسمها الترادف ( المعني الواحد يدل عليه اكثر من لفظ مثل " السعادة و الفرح " ) و الاشتراك ( اللفط الواحد يدل علي اكثر من معني مثل " العيق تطلق علي العين البشرية او عين الماء او الجاسوس او رجل وجيه "  ) . 
<br>و قاعدة : " و كل و صف أوهم التشبيه اوله او فوض وروم تنزيه " يعني صفة من صفات الله تعالي تراها فتظن ان هذه الصفة قد تقودك الي تشبيه الله بخلقه فبمعني التشبيه ده خاطيء عقلاً او نقلاُ ليس كمثله شيء ، و الحل لإما التأويل اي تعريفها يمعني قريب الي الذهن او التفويض اي حقيقة المعاني الي الله . 
<br><br>
<br>الجواب هناك سؤلان في الاصل و هما : 
<br>
<br>غاية الله عزوجل ... لا يوصل الي شيء<br>
لما لا يوصل الي شيء ؟ لان الله عزوجل منزه عن الغايات و البواعث قال تعالي في حديث قدسي : " يا عباجي اني ما خلقتكم لاستكثر بكم من قله ، و لا لآنس بكم من وحشة " 
<br>ما دور الانسان .... سؤال مشروع<br>
و هذا السؤال سوف ينقلنا الي سؤال و مفهوم اخر الا هو النموذج المعرفي ( اجابة اسئلة اربعة ) و هو : 

<br>من انا ؟ خلق من خلق الله تعالي كرمه الله " انسان " ، حيث انه اعطاك القدرة علي الاختيار الا و هو العقل ، لكنه غير تام الاختيار ، لانه اذا كان تام الاختيار اصبح إله ، لذلك تحاسب علي ما تختاره . 
<br>من اين جئت ؟ : جئت من التراب<br>
الفلسفة في الاسلام هي العقيدة<br>
شعر : " خفف الوطأ ما اظن اديم الارض الا من هذه الاجساد - فسر إن استطعت في الارض هونا لا اختيالا علي روفات العباد - صاحي هذه قبورنا تملاُ الارض - فإين القبور من عهد عاد " 


]]></description><link>_zettlenotes\other-notes\03-الدرس-الثالث.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/03 الدرس الثالث.md</guid><dc:creator><![CDATA[انس السلطان]]></dc:creator><pubDate>Thu, 25 Jul 2024 23:59:33 GMT</pubDate></item><item><title><![CDATA[9 نصائح تمنيت لو تعلمتها قبل ال 30]]></title><description><![CDATA[ 
 <br>1.النجاح مهم بس الاهم انك تعرف يعني ايه النجاح بالنسبة لك؟  <br>
<br>النجاح بالنسبالي هو مرضاة الله عز و جل و دخول الجنة  
2.بناء العلاقات لا يقل اهمية عن الشغل نفسه .  
3.ملكش دعوة بالهمبكه ( يعني انك تبين انك بتشتغل و انت في الحقيقة مش بتشتغل )  
4.التحفيز مهم جدا و لكن ما هو التحفيز الصحيح ؟  
<br>التحفيز الصحيح هو التحفيز المبني علي اساس قيم انت مقتنع بيها مش اي كلام و السلام .  
5.ريادة الاعمال و الوظيفة وجهان لعمله واحدة "السعي"  
<br>اختار الذي يتناسب مع طبيعة شخصيتك  
6.الاستعجال هو عدوك الاول!  
<br>احسن حاجه الي تبنيها علي مدي طويل مع استمرارية و صبر  
7.صاحب الملل...صاحب الحاجات اللي بتضايقك  
<br>فكره انك بتعمل الحاجات اللي انت بتستمتع و انت بتعملها ولازم تكون شغوف بيها و كده دي هتأخرك كتير و هتضيع منك فرص كتير في الحياة .  
<br>يعني اعمل الي انت مسؤول منه ، مش اللي انت عايز تعمله  
8.احلم احلام كبيرة... بس خلي احلامك شاملة كاملة  
<br>يعني تشمل الاخرة و الدنيا ... مفيش حاجه اسمها ان اي حد يقدر يعمل اي حاجه.  
<br>الشعور بالرضا و لاتضع الارض و الدنيا هي التحدي الاكبر و هي الاساس لأ ...  
9.ايمانك و دينك مركزية حياتك  
<br>خلي ايمانك و دينك هو مركزية تحركك ، يرضي الله و لا لا يرضي الله خلي ده اول فلتر ليك ، لو مش موجود يبقي حياتك ملهاش لازمه و هتشقي كتير في حياتك .
]]></description><link>_zettlenotes\other-notes\9-نصائح-تمنيت-لو-تعلمتها-قبل-ال-30.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/9 نصائح تمنيت لو تعلمتها قبل ال 30.md</guid><dc:creator><![CDATA[كريم اسماعيل]]></dc:creator><pubDate>Tue, 23 Jul 2024 16:36:06 GMT</pubDate></item><item><title><![CDATA[cross validation]]></title><description><![CDATA[ 
 <br>dev set =  validation set = development set <br>definition : to check accuracy of different models <br>الطريقة كالأتي : <br>we select the best model between some models using training and dev sets after that using testing set to check the actual performance of the model . <br>so ,<br>
training set : used to train the model .<br>
validation set : used to tune hyperparameters and select the best model .<br>
test set : used only once for the final evaluation after the model is fully trained and tuned  . <br><br>Regularization<br>
reduce overfitting by add a penalty<br>
Cross validation<br>
try to add advise and lead the model to correct roadmap (tuning parameters)<br>
is a check point to ensure the model is learning effectively and generalizing well to new data ]]></description><link>_zettlenotes\other-notes\cross-validation.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/cross validation.md</guid><pubDate>Wed, 31 Jul 2024 00:50:56 GMT</pubDate></item><item><title><![CDATA[Git commands]]></title><description><![CDATA[ 
 <br>
<br>𝐠𝐢𝐭 𝐝𝐢𝐟𝐟: Show file differences not yet staged.  
<br>𝐠𝐢𝐭 𝐜𝐨𝐦𝐦𝐢𝐭 -𝐚 -𝐦 "𝐜𝐨𝐦𝐦𝐢𝐭 𝐦𝐞𝐬𝐬𝐚𝐠𝐞": Commit all tracked changes with a message.  
<br>𝐠𝐢𝐭 𝐜𝐨𝐦𝐦𝐢𝐭 --𝐚𝐦𝐞𝐧𝐝: Modify the last commit.  
<br>𝐠𝐢𝐭 𝐬𝐭𝐚𝐭𝐮𝐬: Show the state of your working directory.  
<br>𝐠𝐢𝐭 𝐚𝐝𝐝 𝐟𝐢𝐥𝐞_𝐩𝐚𝐭𝐡: Add file(s) to the staging area.  
<br>𝐠𝐢𝐭 𝐜𝐡𝐞𝐜𝐤𝐨𝐮𝐭 -𝐛 𝐛𝐫𝐚𝐧𝐜𝐡_𝐧𝐚𝐦𝐞: Create and switch to a new branch.  
<br>𝐠𝐢𝐭 𝐜𝐡𝐞𝐜𝐤𝐨𝐮𝐭 𝐛𝐫𝐚𝐧𝐜𝐡_𝐧𝐚𝐦𝐞: Switch to an existing branch.  
<br>𝐠𝐢𝐭 𝐜𝐡𝐞𝐜𝐤𝐨𝐮𝐭 &lt;𝐜𝐨𝐦𝐦𝐢𝐭&gt;: Switches the working directory to a specific commit.  
<br>𝐠𝐢𝐭 𝐩𝐮𝐬𝐡 𝐨𝐫𝐢𝐠𝐢𝐧 𝐛𝐫𝐚𝐧𝐜𝐡_𝐧𝐚𝐦𝐞: Push a branch to a remote.  
<br>𝐠𝐢𝐭 𝐩𝐮𝐥𝐥: Fetch and merge remote changes.  
<br>𝐠𝐢𝐭 𝐟𝐞𝐭𝐜𝐡: Fetch changes from the remote repository without merging.  
<br>𝐠𝐢𝐭 𝐫𝐞𝐛𝐚𝐬𝐞 -𝐢: Rebase interactively, rewrite commit history.  
<br>𝐠𝐢𝐭 𝐫𝐞𝐛𝐚𝐬𝐞 𝐛𝐫𝐚𝐧𝐜𝐡_𝐧𝐚𝐦𝐞: Rebase the current branch onto another branch.  
<br>𝐠𝐢𝐭 𝐜𝐥𝐨𝐧𝐞: Create a local copy of a remote repo.  
<br>𝐠𝐢𝐭 𝐦𝐞𝐫𝐠𝐞: Merge branches together.  
<br>𝐠𝐢𝐭 𝐥𝐨𝐠 --𝐬𝐭𝐚𝐭: Show commit logs with stats.  
<br>𝐠𝐢𝐭 𝐬𝐭𝐚𝐬𝐡: Stash changes for later.  
<br>𝐠𝐢𝐭 𝐬𝐭𝐚𝐬𝐡 𝐩𝐨𝐩: Apply and remove stashed changes.  
<br>𝐠𝐢𝐭 𝐬𝐡𝐨𝐰 𝐜𝐨𝐦𝐦𝐢𝐭_𝐢𝐝: Show details about a commit.  
<br>𝐠𝐢𝐭 𝐫𝐞𝐬𝐞𝐭 𝐇𝐄𝐀𝐃~1: Undo the last commit, preserving changes locally.  
<br>𝐠𝐢𝐭 𝐛𝐫𝐚𝐧𝐜𝐡 -𝐃 𝐛𝐫𝐚𝐧𝐜𝐡_𝐧𝐚𝐦𝐞: Delete a branch forcefully.  
<br>𝐠𝐢𝐭 𝐫𝐞𝐬𝐞𝐭: Undo commits by moving branch reference.  
<br>𝐠𝐢𝐭 𝐫𝐞𝐯𝐞𝐫𝐭 𝐜𝐨𝐦𝐦𝐢𝐭_𝐢𝐝: Create a new commit that undoes the changes of a specific commit.  
<br>𝐠𝐢𝐭 𝐜𝐡𝐞𝐫𝐫𝐲-𝐩𝐢𝐜𝐤 𝐜𝐨𝐦𝐦𝐢𝐭_𝐢𝐝: Apply changes from a specific commit.  
<br>𝐠𝐢𝐭 𝐛𝐫𝐚𝐧𝐜𝐡: Lists branches.  
<br>𝐠𝐢𝐭 𝐫𝐞𝐬𝐞𝐭 --𝐬𝐨𝐟𝐭 𝐇𝐄𝐀𝐃^: Undo the last commit, but keep the changes.  
<br>𝐠𝐢𝐭 𝐫𝐞𝐬𝐞𝐭 --𝐡𝐚𝐫𝐝: Resets everything to a previous commit, erasing all uncommitted changes.  
<br>𝐠𝐢𝐭 𝐛𝐫𝐚𝐧𝐜𝐡 --𝐬𝐞𝐭-𝐮𝐩𝐬𝐭𝐫𝐞𝐚𝐦-𝐭𝐨 𝐫𝐞𝐦𝐨𝐭𝐞_𝐛𝐫𝐚𝐧𝐜𝐡: Sets the upstream branch to the specified remote branch.
]]></description><link>_zettlenotes\other-notes\git-commands.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/Git commands.md</guid><pubDate>Tue, 23 Jul 2024 14:34:17 GMT</pubDate></item><item><title><![CDATA[how to create a progress bar in google sheet ]]></title><description><![CDATA[ 
 <br>=REPT(char(129001),(B2/C2100)/10)&amp; REPT(CHAR(11036) , (100 -(B2/C2100) ) /10) &amp; ROUND((B2/C2*100) , 2 ) &amp; "%" &amp; if(B2=C2 , "🎉" , "")]]></description><link>_zettlenotes\other-notes\how-to-create-a-progress-bar-in-google-sheet-.md\how-to-create-a-progress-bar-in-google-sheet-.md.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/how to create a progress bar in google sheet .md</guid><pubDate>Tue, 23 Jul 2024 16:32:52 GMT</pubDate></item><item><title><![CDATA[how to self study technical things]]></title><description><![CDATA[ 
 <br>
<br>learn Just enough  
<br>start with projects  
<br>iterate  
<br>accountability  
<br>
<br>ما هي العواقب المتربتة علي ترك هذا العمل  
<br>
<br>take notes only of high level concepts and focus on understanding
]]></description><link>_zettlenotes\other-notes\how-to-self-study-technical-things.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/how to self study technical things.md</guid><dc:creator><![CDATA[Tina Huang,Andrew ng]]></dc:creator><pubDate>Tue, 23 Jul 2024 16:13:32 GMT</pubDate></item><item><title><![CDATA[how to take notes for technical things]]></title><description><![CDATA[ 
 <br>
<br>يعتبر كل الوسائل المشهور الي عرفتها زي كورنيل و المايند ماب و الاوت لاين ، الهدف منها بشكل رئيسي هو الحفظ و استرجاع المعلومة .... و لكن بالنسبة للمجالات مثل البرمجه و الرياضيات ليس الهدف منها هو الحفظ و لكن الهدف منها هو القدرة علي تطبيق المفاهيم التي تعلمتها و استنباط مفاهيم اخري ، لذلك فالمقدمة تقول انها لا تستخدم تلك الاساليب اثناء تعلمها البرمجه<br>
Tools  
<br>
<br>pen and paper for me  
<br>pre-learning<br>
2. capturing the general concept or indexing on the topics and what is told  <br>
<br>يعني عمل زي ملخص سريع للموضوعات الي هيتكلم عليها الكورس قبل البدء فيه و ذلك بيساعد علي زيادة تركيزك في الكورس .  
<br>Taking- notes  <br>
<br>create framework of references  
<br>
<br>اخذ ملاحظات علي المفاهيم و المصطلحات و ذلك لبناء مثل bird view عشان تعرف تربط ما بين المفاهميم و بعضها و تعرف الغرض من كل واحدة فيهم اما بالنسبة للكيفية فدي امرها سهل ( من خلال البحث علي جوجل )  
<br>و كتابة الاوقات او الوقت الزمني لمعلومة مثلا صعبا بالنسبة لك و تتوقع انك سترجع اليها في المستقبل ، فيفضل الزمن الذي شرح فيها المحاضر تلك المعلومة.... لكي يسهل العودة اليها في المستقبل  
<br>
<br>write your own insights and connections  
<br>
<br>يعني اكتب امثلة او اشياء تقرب لك فهم المعلومة او الغرض من كلام المحاضر .... يعني المعلومة الحالية باخري لديك و ذلك لتعزيز الفهم لديك و كذلك من سهولة الحفظ فيما بعد<br>
3.write a full example cover the all concepts in the video  
<br>اكتب مثال او مشروع او مشكلة توضح فيها كيفية ربط المعلومات الي انت تعلمتها خلال المحاضرة مع بعض<br>
4.create readme file for the project you will create , for after long interval when you back to the project remember the contents of the project and how this step is done and so on
]]></description><link>_zettlenotes\other-notes\how-to-take-notes-for-technical-things.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/how to take notes for technical things.md</guid><dc:creator><![CDATA[Tina Huang]]></dc:creator><pubDate>Tue, 23 Jul 2024 16:12:14 GMT</pubDate></item><item><title><![CDATA[how to take notes project ]]></title><description><![CDATA[ 
 <br><br>نعم هناك فرق اكيد بينهما و الفرق يكمن في ان الفائدة هي القيمة التي تستمد من الملحوظة او المعلومة التي تأخذها ، و عملية تجميع  المعلومات و تسجيل الخواطر تجاه تلك المعلومات هي الملحوظة .  <br><br>
<br>في البداية الملاحظات لها اهمية كبيره في تعميق فهمك للمحتوي الذي تدرسة حيث انك تقوم بطرح الافكار التي تأتي في ذهنك و من ثم تنتقي منها الصالح و تحفظة و الخاطيء تقوم بتقويمه و تعديله و اعادة النظر فيه و هذا يساعدك علي فهم المحتوي بشكل عميق . (الملاحظات هي المؤثر الاول لك لمعرفة مدي فهمك لملعلمة المشروحة ) .
<br><br>هناك طرق كثيرة لاخذ الملاحظات و تبدأ من حيث الادوات ديجيتال ( obsidian - Notion )  او ورقي .<br>
و طريقة اخذ الملحوظة نفسها (cornel - feyman - mind map - outline ) ، و كل طريقة لها ما لها و عليها ما عليها .<br>
الاستذكار الفعال و التكرار المتباعد <a data-href="active recall" href="\active recall" class="internal-link" target="_self" rel="noopener">active recall</a> and <a data-href="space Repetition" href="\space Repetition" class="internal-link" target="_self" rel="noopener">space Repetition</a><br><br>اذا بعد معرفة الطريقة المناسبة لك في اخذ الملاحظات دعنا ننطلق سويا لمعرفة كيفية تنظيم هذه الملاحظات بحيث يسهل عليك بعد ذلك العودة اليها و  تحقيق الفائدة منها بقدر الاستطاعة . <br><br><br><br><br>من حيث طريقة التدوين انا افضل استخدام الطريقة الديجتال في اغلب الاحيان ، و لكن الجأ في بعض الاحيان الي الطريقة الورقية في بعض الامور مثل حل المسائل الرياضية او رسم رسمه سريعة لزيادة ادراكي لمعلومة معينة  ، و سبب تفضيلي للطريقة الديجيتال هو قدرتي علي تنظيم المعلومات و سهولة الرجوع لها و تقليل عدد الورق الازم للمذاكرة و كذلك تطبيق المبادئ الفعالة في الدراسة حيث اقوم بتدوين الملاحظات و بعد ذلك اقوم بعمل اسئلة علي الدرس كامل و حل المسائل ان وجدت المتعلقة بالدرس و اعود الي الملاحظة مره اخري بعد فترة من الزمن و مراجعتها  ، " فالعامل الاساسي و الجدير بالاهتمام ليس الطريقة بل الكيفية المستخدمة " و لذلك لزيادة لتحقيق الاستفادة القصوي من الملاحظة عليك بالاستذكار الفعال و من ثم التكرار المتباعد ، و هاتان الطريقتان غالبا الهدف الاكبر منها او النسبة الاكبر منها من اجل زيادة نسبة حفظك للمعلومات و جزء ايضا من اجل زيادة الفهم "فتكرار المطالعة علي الشيء - علي فترات متباعدة -  يزيد من مدي ادراكك له "  <br><br>LINKS TO THIS PAGE <br>Dataview: No results to show for list query.]]></description><link>_zettlenotes\other-notes\how-to-take-notes-project-.md\how-to-take-notes-project-.md.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/how to take notes project .md</guid><dc:creator><![CDATA[tarek mohammed]]></dc:creator><pubDate>Mon, 29 Jul 2024 02:39:07 GMT</pubDate></item><item><title><![CDATA[Index - data science]]></title><description><![CDATA[ 
 <br><br><a data-href="machine learning specialization" href="\_zettlenotes\programming-notes\ai-notes\machine-learning-specialization.html" class="internal-link" target="_self" rel="noopener">machine learning specialization</a><br><a data-href="deep learning specialization" href="\_zettlenotes\programming-notes\ai-notes\deep-learning-specialization.html" class="internal-link" target="_self" rel="noopener">deep learning specialization</a>]]></description><link>_zettlenotes\other-notes\index-data-science.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/Index - data science.md</guid><pubDate>Sun, 28 Jul 2024 23:13:54 GMT</pubDate></item><item><title><![CDATA[Index - Productivity]]></title><description><![CDATA[ 
 <br>يتحدث عن الانتاجية و الاساليب الصحيحة  <br><a data-href="PPV System method" href="\_zettlenotes\other-notes\ppv-system-method.html" class="internal-link" target="_self" rel="noopener">PPV System method</a> - explain the PPV system  <br><a data-tooltip-position="top" aria-label="كتاب عام ال 12 اسبوع" data-href="كتاب عام ال 12 اسبوع" href="\_zettlenotes\other-notes\كتاب-عام-ال-12-اسبوع.html" class="internal-link" target="_self" rel="noopener">كتاب عام ال 12 اسبوع</a> - يتحدث عن مبدأ تقسيم الهدف الكبير  و تغيير رؤية لتنظيم الاهداف من 3 شهور الي 12 اسبوع . <br><a data-href="كيف تنجز في هذا العالم المختل" href="\_zettlenotes\other-notes\كيف-تنجز-في-هذا-العالم-المختل.html" class="internal-link" target="_self" rel="noopener">كيف تنجز في هذا العالم المختل</a> - يشرح المبادئ الاساسية في الانجاز . <br><a data-href="عوامل النجاح ( مثلث النجاح )" href="\_zettlenotes\other-notes\عوامل-النجاح-(-مثلث-النجاح-).html" class="internal-link" target="_self" rel="noopener">عوامل النجاح ( مثلث النجاح )</a> - سعي عمل صالح مناسب لقدراتك الحالية . <br><a data-href="اهداف ام عادات" href="\_zettlenotes\other-notes\اهداف-ام-عادات.html" class="internal-link" target="_self" rel="noopener">اهداف ام عادات</a>  -  فكرة عند التخطيط  هل علي اساس العادات ام الاهداف ]]></description><link>_zettlenotes\other-notes\index-productivity.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/Index - Productivity.md</guid><pubDate>Sun, 28 Jul 2024 20:58:30 GMT</pubDate></item><item><title><![CDATA[Index - برنامج اعادة تشغيل الذات]]></title><description><![CDATA[ 
 <br><a data-href="مهارة التدوين و التلخيص و الارشفة" href="\_zettlenotes\other-notes\مهارة-التدوين-و-التلخيص-و-الارشفة.html" class="internal-link" target="_self" rel="noopener">مهارة التدوين و التلخيص و الارشفة</a> - بيشرح ماهية التدوين و ما هو التدوين و رد علي معضلة ورقي ولا ديجيتال ؟ <br><a data-href="من انا ؟ و لما انا ؟ لماذا خلقت" href="\_zettlenotes\other-notes\من-انا-؟-و-لما-انا-؟-لماذا-خلقت.html" class="internal-link" target="_self" rel="noopener">من انا ؟ و لما انا ؟ لماذا خلقت</a> - تتكلم عن كيفية تحديد الاهداف و مثلث ( الوظيفة - الهوية - الهدف  )<br><a data-href="صوتية الصحة" href="\_zettlenotes\other-notes\صوتية-الصحة.html" class="internal-link" target="_self" rel="noopener">صوتية الصحة</a> - يتحدث عن مفهوم الصحة و معني كلمة الرياضة . ]]></description><link>_zettlenotes\other-notes\index-برنامج-اعادة-تشغيل-الذات.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/Index - برنامج اعادة تشغيل الذات.md</guid><pubDate>Thu, 25 Jul 2024 22:34:06 GMT</pubDate></item><item><title><![CDATA[Index - تصحيح التصورات و المعتقدات]]></title><description><![CDATA[ 
 <br>مجموعة من النصائح الحياتية القيمة و بعض الافكار التي من شأنها تصحيح تصوراتك تجاه الحياة و الناس و ايضا تجاه فهمك للدين بشكل سوي و صحيح إن شاء الله . <br><a data-href="9 نصائح تمنيت لو تعلمتها قبل ال 30" href="\_zettlenotes\other-notes\9-نصائح-تمنيت-لو-تعلمتها-قبل-ال-30.html" class="internal-link" target="_self" rel="noopener">9 نصائح تمنيت لو تعلمتها قبل ال 30</a> - بعض النصائح المهمة تجاه فهمك للحياة و المجتمع و العلاقة بينك و بين الله . <br><a data-href="الذات" href="\_zettlenotes\other-notes\الذات.html" class="internal-link" target="_self" rel="noopener">الذات</a> - يتحدث عن النفس من منظور موضوعي و ايضا جزء ديني في الموضوع لكي تستطيع التعامل مع هذا الشيء الذي بداخلك عليك في البداية فهمه . <br><a data-href="المسافة بين رغبتنا و انجازاتنا" href="\_zettlenotes\other-notes\المسافة-بين-رغبتنا-و-انجازاتنا.html" class="internal-link" target="_self" rel="noopener">المسافة بين رغبتنا و انجازاتنا</a> - من اكتر الملاحظات التي كونت عندي مبدأ لكل شيء يجب ان يكون له معيار محدد يحكمة و يضبطه . ]]></description><link>_zettlenotes\other-notes\index-تصحيح-التصورات-و-المعتقدات.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/Index - تصحيح التصورات و المعتقدات.md</guid><pubDate>Thu, 25 Jul 2024 22:04:31 GMT</pubDate></item><item><title><![CDATA[Index - ما لا يسع المسلم جهله]]></title><description><![CDATA[ 
 <br>روابط لسلسلة دروة ما لا يسع المسلم جهله لانس السلطان فك الله اسره ، و بعض الاضافات و التعليقات عليها . <br><a data-href="01 الدرس الاول" href="\_zettlenotes\other-notes\01-الدرس-الاول.html" class="internal-link" target="_self" rel="noopener">01 الدرس الاول</a> - يتحدث عن مقدمة عن العلوم الشرعية و فائدة كل علم من العلوم الكبار مثل ( الفقه و العقيدة و التزكية ) . <br><a data-href="02 الدرس الثاني" href="\_zettlenotes\other-notes\02-الدرس-الثاني.html" class="internal-link" target="_self" rel="noopener">02 الدرس الثاني</a> - يتحدث عن او العلوم الا و هو العقيدة و انها تعد الفلسفة في الاسلام و كيفية التعرف علي الخالق سبحانه و تعالي . <br><a data-href="03 الدرس الثالث" href="\_zettlenotes\other-notes\03-الدرس-الثالث.html" class="internal-link" target="_self" rel="noopener">03 الدرس الثالث</a> - استكمال لما قد قيل في الدرس الثاني عن العقيدة و التحدث اكثر عن صفات الخالق عزوجل و الرد علي بعض الاشكالات المتعلقة بمسألة لماذا خلقت و بعض صفات الله . ]]></description><link>_zettlenotes\other-notes\index-ما-لا-يسع-المسلم-جهله.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/Index - ما لا يسع المسلم جهله.md</guid><pubDate>Fri, 26 Jul 2024 00:10:05 GMT</pubDate></item><item><title><![CDATA[intro to digital marketing]]></title><description><![CDATA[ 
 <br><br>
<br>SEO : search engine optimization<br>
" تحسين وجودك علي الانترنت في نتائج البحث "
<br>SEM : search engine marketing<br>
" نفس ال seo بس للاعلانات المدفوعة "
<br>SMM : search media marketing<br>
"خاصة بالتسويق علي منصات التواصل الاجتماعي " 
<br>content marketing : التسويق بالمحتوي زي عمل موقع خاص بيك 
<br>Email marketing 
<br>video marketing 
<br>backlinks : تكسب شهرة من خلال مواقع كبيرة ، و ده لما يكون المحتوي بتاعك ذو جودة عالية ، او من خلال انك تتدفعلهم 
<br>google analytics : عشان تشوف الاداء بتاع الموقع بتاعك 
<br>Affiliation : التسويق بالعمولة " يعني انك تروح لصفحة شخص معين و كل زيارة بتاخد عليها نسبة " 
<br><br><br>
<br>Domain : لازم يكون معبر عن الصفحة 
<br>Hosting : لازم يكون معروف عشان هيفرق في سرعة الموقع 
<br>The design (UX) : بيفضل الاهتمام به في الاول مقترح recommend wallet website for design the website  
<br>google search console : منصة الادوات الخاصة للاشراف علي الموقع 
<br>competitors analysis : دراسة المنافسين  
<br>keywords : الكلمات المفتاحية 
<br>content calendar : خطة المحتوي 
<br>the URL : لازم يكون بيعبر عن محتوي الموقع 
<br>The review : مراجعة المحتوي و الموقع بشكل عام 
]]></description><link>_zettlenotes\other-notes\intro-to-digital-marketing.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/intro to digital marketing.md</guid><pubDate>Thu, 25 Jul 2024 22:17:18 GMT</pubDate></item><item><title><![CDATA[learning curves]]></title><description><![CDATA[ 
 <br>base line : what is the level of error you can reasonably hope to get to ? <br><br><img alt="Pasted image 20240731040247.png" src="\lib\media\pasted-image-20240731040247.png"><br>كلما زاد عدد الامثلة كلما زاد نسبة الخطأ الخاصة بال train لانه يصعب عليه عمل fit لكل الامثلة .<br>
و لكن في حالة ان ال model به high bias فإن زيادة حجم ال training set لن يؤثر علي ال model .<br>
if a learning algorithm suffers from high variance , getting more training data is likely to help . ]]></description><link>_zettlenotes\other-notes\learning-curves.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/learning curves.md</guid><pubDate>Wed, 31 Jul 2024 01:05:39 GMT</pubDate><enclosure url="lib\media\pasted-image-20240731040247.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240731040247.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[loop of ML development]]></title><description><![CDATA[ 
 <br><img alt="Pasted image 20240731041721.png" src="\lib\media\pasted-image-20240731041721.png"><br><br> = 500<br>
ممكن مثلا ال model يصنف 100 من اصل 500 غلط ، فانت ك ml developer هتصنفهم انت بنفسك و تحاول تعدل في ال Architectures  بتاعة ال model<br>
Note that : Adding more data cause to increase time cost .<br>
فبدل ما اضيف داتا for every thing لا ، احنا هنضيف داتا data types where error analysis  . ]]></description><link>_zettlenotes\other-notes\loop-of-ml-development.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/loop of ML development.md</guid><pubDate>Wed, 31 Jul 2024 01:24:43 GMT</pubDate><enclosure url="lib\media\pasted-image-20240731041721.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240731041721.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[oct-29-2023 ( تحليل الشخصية )]]></title><description><![CDATA[ 
 <br><img alt="Pasted image 20240723191454.png" src="\lib\media\pasted-image-20240723191454.png">]]></description><link>_zettlenotes\other-notes\oct-29-2023-(-تحليل-الشخصية-).html</link><guid isPermaLink="false">_ZettleNotes/other Notes/oct-29-2023 ( تحليل الشخصية ).md</guid><pubDate>Tue, 23 Jul 2024 16:14:56 GMT</pubDate><enclosure url="lib\media\pasted-image-20240723191454.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240723191454.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[PPV System method]]></title><description><![CDATA[ 
 <br>Summary<br>
This video provides an overview of the Pillars, Pipelines &amp; Vaults system in Notion, which is a comprehensive life operating system. It explains how these three categories relate to each other and the benefits of integrating them. <br>Highlights<br>
🏛️ Pillars, Pipelines &amp; Vaults Introduction to a life operating system in Notion.<br>
🤝 Focus and Alignment The system helps achieve focus, alignment, and knowledge resurfacing.<br>
📚 Pillars Horizontal organizational categories that categorize different aspects of life (e.g., growth, home life, business).<br>
⚙️ Pipelines Action-oriented process flow for tasks, projects, goals, and values.<br>
🗄️ Vaults Knowledge management for capturing and organizing information, ideas, and insights.<br>
🔁 Cycle Reviews Periodic review cycles to maintain alignment and track progress.<br>
🎯 Command Center Top-level dashboard for focus and alignment in the system.]]></description><link>_zettlenotes\other-notes\ppv-system-method.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/PPV System method.md</guid><pubDate>Mon, 29 Jul 2024 19:45:56 GMT</pubDate></item><item><title><![CDATA[which is better pen and paper or digital note taking]]></title><description><![CDATA[ 
 <br>
<br>
لم تظهر الدراسات العلميه فرق كبير ما بين الكتابة الورقيه و الكتابة بلوحة المفاتيح علي الكمبيوتر… لكن المفتاح هنا في المراجعة المستمره علي فترات متباعدة&nbsp;

<br>
لذلك هناك اسباب لعدم اختيار الورقة و القلم و الاعتماد علي الطريقة الديجيتال :&nbsp;
1.poor storage and management&nbsp;

<br>
<br>
No search function&nbsp;

<br>
Poor image handling&nbsp;

<br><br>
<br>لا مفر من وجود عيوب في الطريقة الديجيتال ايضا :&nbsp;
<br>
<br>عرضة اكبر للمشتتات&nbsp;
<br>نقل ما يقوله الاستاذ بالحرف دون التعديل عليه ، و هذا نوع من التعلم السلبي ، لذلك يفضل اتباع طريقة في تدوين الملاحظات مثل : cornel method , outline method
]]></description><link>_zettlenotes\other-notes\which-is-better-pen-and-paper-or-digital-note-taking.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/which is better pen and paper or digital note taking.md</guid><dc:creator><![CDATA[tarek mohammed]]></dc:creator><pubDate>Tue, 23 Jul 2024 19:38:03 GMT</pubDate></item><item><title><![CDATA[اشهر 6 قوانين للانجاز و الانتاجية ]]></title><description><![CDATA[ 
 <br>
<br>pareto principle : 80/20 rule 
<br>parkinson's law : اي مهمة هتتسع للوقت الي انت تديهولها ( لو محددتش معاد للمذاكرة اذا الوقت لا نهائي ، يبقي مش هتذاكر ) 
<br>carlson's law : اي مهمة يتم مقاطعتك فيها تكون اقل في الجودة و اقل في الكفاءة 
<br>I llich's law : فيه عتبة  او حد معين بتوصل لية كفاءة الانسان و بعدين بتقل 
<br>laborit's law : ابدأ بالاصعب في اليوم 
<br>hafstabter's law : اي مهمة هتاخد منك وقت اكثر من المتوقع ليه ، يعني لو فيه مهمة انت ادتها وقت معين حاول تزود نسبة 30% مثلا من الوقت اضافة علي الي انت حاطة 
]]></description><link>_zettlenotes\other-notes\اشهر-6-قوانين-للانجاز-و-الانتاجية-.md\اشهر-6-قوانين-للانجاز-و-الانتاجية-.md.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/اشهر 6 قوانين للانجاز و الانتاجية .md</guid><dc:creator><![CDATA[علي محمد علي,احمد ابو زيد]]></dc:creator><pubDate>Sat, 27 Jul 2024 06:34:19 GMT</pubDate></item><item><title><![CDATA[التجريب]]></title><description><![CDATA[ 
 <br>التجريب يساعدك علي فهم الامور بشكل اعمق لأن مفيش كتاب هيكون بيشرح كل صغيرة و كبيرة في المجال لان ذلك سوف يضعب المادة العملية و يضخمها لذلك هو الحل الانسب . <br>كذلك التجريب يساعد علي زيادة الفهم <br>دائماً ما يرافق التجريب امران .. الفشل و التعلم <br>الهدف الاساسي من التجربة ليس النجاح ، بل لتعلم بذلك مادمت تعلمت من التجربة فأنت علي الطريق الصحيح . ]]></description><link>_zettlenotes\other-notes\التجريب.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/التجريب.md</guid><dc:creator><![CDATA[علي محمد علي]]></dc:creator><pubDate>Fri, 26 Jul 2024 16:27:03 GMT</pubDate></item><item><title><![CDATA[الذات]]></title><description><![CDATA[ 
 <br>
<br>النفس هي محل الشهوي و الانفعالات  
<br>العقل هو سائق عند النفس  
<br>الغرفة التي تحتوي كل هذا اسمه الذات ... من حيث انفعالاتك و سلوكك .  
<br><br>
<br>من وجهة نظري و علي قدر ما انا فاهم أن القلب هو محل او هو وعاء يجب يملأ لاما بهوي النفس و الشيطان أو بالعقل و الصلاح و التقوي و القرآن الكريم ... القلب أو الروح أو الجانب المعنوي يجب أن يغزي لاما من خلال الهوي او من خلال العقل و الوحي و السنه الصحيحه  
<br>و اعتقد ان النفس هي دابتك التي تقودها في حياتك و لها اكثر من شكل علي حسب ما تغزيها و طريقة تعاملك معها ( نفس مطمئنه و نفس لوامه و نفس خبيثه و نفس إمارة بالسوء ) و كل هذه الاشكال هي نتيجه من الأصل الا و هو القلب أو الروح كلما غزيت هذا الجانب بالصالح من العلم و العمل كلما عودت نفسك علي ما هو طيب و ابعدتها عن كل ما فيه هوي لها ضار غير نافع و مرنتها علي ما هو صالح لها و تعاملت مع نفسك علي انك لها ناصح محب و النفس علي ما تعودها و الذات الشكل النهائي الناتج من الصراع بين العقل و الهوي
]]></description><link>_zettlenotes\other-notes\الذات.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/الذات.md</guid><dc:creator><![CDATA[ياسر الحزيمي,ابراهيم الخليفي]]></dc:creator><pubDate>Tue, 23 Jul 2024 16:22:06 GMT</pubDate></item><item><title><![CDATA[الطريق ]]></title><description><![CDATA[ 
 <br>
<br>يفضل في البداية كده التعمق في كل مجال ، هو لا ينصح بالبداية من كورس cs50 لانه لا يتعمق في اي جزء و انما يعطيك جزء بسيط جدا من كل مجال .  
<br>الدكتور بيقول انه مش بيفضل و مش لازم انك تخش الكلية و تبقي عارف انت هتتطلع ايه بل هو لا يحبز هذا الفعل لانه كده انت بضيق ال scale علي نفسك و بتفوت فرص علي نفسك بكونك خنقت نفسك بتخصص واحد بس ، و لكن هو بيفضل انك تبدأ تتعلم الاساسيات و انت ماشي في الطريق تشوف كل تخصص بيتكلم عن ايه و تشوف ايه التخصص الي انت ميال له .  
<br>الدكتور بيقول برده دلوقتي لو فيه تخصص هو المستقبل و لكن ملوش شغل دلوقتي يعني احتمال يكون ليه مستقبل قدام و ليه شغل زي مثلا في الوقت ده هو الذكاء الاصطناعي ، هو بيقول انه يفضل ان الشغل الي انت شاطر فيه هتلاقي لنفسك شغلانه ، و لكن متروحش تتعلم مجال ملوش شغل في بلدك كده نسبة عملك هتقل ، لذلك من وجة نظري هو تعلم المجال الي ليه شغل اكتر في بلدك و المجال الي ليه مستقبل ابقي بص عليه من فتره الي الاخري و خد فيه بعض الكورسات .  
<br>الدكتور كمان بيقول كمان انه في بداية تعلمك للبرمجه و خاصة في فترة تعلم الاساسيات لازم تتمرن كتير جدا و تحل مسائل كتير عشان تثبت المعلومة + تفهم المواضيع بشكل اعمق مش مجرد امر نظري فقط .  
<br>🟩الدكتور بيقول انه عشان تتقدر تقول انك معاك الاساسيات لازم يكون متعلم 9 مواد مهمين في cs و يحبب تدور عليهم مع نفسك علي الانترنت :  
<br>
<br>programming and OOP  
<br>Data structure and Algorithms [important]  
<br>
<br>junior sheet (900 questions ) ، كلما حليت اكتر كلما مستواك بيعلي و لكن لا يقل عن 500 مسألة  
<br>الحاجات الي جيه انت مش محتاج تتعمق فيها جدا ، لكن محتاج تاجد كورس و تعمل pratice عليه .  
<br>
<br>design patterns and software engineering  
<br>database  
<br>Networking  
<br>Operating system  
<br>
<br>بعدين الدكتور بيدي تطبيق رائع علي الي شرحة قبل كده و مثال علي ذلك هو google map :  
<br>بيقولك ان التيم بتاع google map تيم كبير جدا و متحتاج بتعلموا organization و هنا بيظر دول ال software engineering كمادة .  
<br>و بعدين عشان تمثل شكل الطرق و هكذا ده هيخش في حاجه اسمها ال Graph theory و دي حاجه تبع ال data structure and algorithms و الموضوع هيكون اكثر تعقيدا عن كونك هتطبق ال graph theory و خلاص و لكن هتحتاج حاجات اخري مع النظريه و هنا يظهر دول ال oop and design pattern  
<br>و بعد كده محتاجين ال database عشان تعمل retrieve للبيانات بتاعتك .  
<br>و كمان تظهر قيمة مادة الالجوريزم عشان تحدد الطرق الممكنه للوصول من نقطة A الي نقطة B مثلا .  
<br>و لاحظ كمان ان الشبكات ممكن تلاقي الي شبكته ضعيفة و الي شبكته قوية و هكذا يعني بالبلدي كده سرعة النت او جودة الشبكه بتتختلف من مكان للاخر فلازم اخد الكلام ده في الحسبان و هنا تظهر فائدة مادة ال Networking معايا في الشغل .  
<br>عرفت بقي اهمية الاساسيات ايه ؟؟ّ!  
<br>🟩خلاص عرفت الاساسيات و اتمكنت منها الحمد لله عايز بقي تنزل السوق و تبدأ تشتغل تعمل ايه ؟  
<br>الدكتور بيقول من اشهر الوظائف في السوق هيا : front end and back end ، لان اي تطبيق محتاج interface يتعامل معاه و backend عشان يقوملك بالحسبات في الخلفية .  
<br>و هنا الدكتور بيقول ان ال front end and back end ممكن يكون web or mobile or desktop  
<br>machine learning : موجود في السوق المصري و لكن مش بينمي بسرعة كافية لمواكبة الاحداث.  
<br>data scientist and analysis : برده موجود في السوق المصري و لكن موجود بنسبة اكبر من ال machine learning و كمان بيكبر عنه بصورة كويسة و لكن دايما هتكون الفرص الاكبر في ال front end and back end  
<br>و هنا الدكتور بيقول نصيحة مهمه جدا الا و هي ان المجالات الي فيها عدد قليل من المبرمجين هتكون المنافسة اكبر عشان كده لازم تثبت نفسك و تتعمق اكتر في المجال عشان يكون ليك شغل في المجال ده .  
<br>ال embeded analysis : موجود و الفرص المتاحة غير كافية لعدد الخريجين في مصر.  
<br>cyber security : المجال ده هيزيد الوعي فيه اكبر و اكبر ، و مرتباته كويسه.  
<br>مرتبك في السوق مش بيتحدد علي بناء صعوبة الشغل الي بتعملها و لكن مرتبك في السوق بيتحدد علي مبدأ العرض و الطلب ، يعني لو الشركات عارضين اماكن شغل لمجال معين و لكن الاقبال عليها مش كبير جدا من قبل المبرمجين يبقي المرتب هيكون عالي .  
<br>مرتبات ال fresh graduate بتتغير من وقت للتاني و خاصة في مصر ، و لكن خد بالك متعود نفسك علي الاستثناءات .  
<br>-ازاي اعرف ايه الي في السوق ؟؟  <br>
<br>عن طريق انك تعمل جوجل علي الانترنت  
<br>مثال : java backend developer job Egypt  
<br>machine learning track :  
<br>🟥Basics : التسع مواد الي فاتو مهمين برده ، بس لو مستعجل شوف oop + graph theory from alogrithms ، لكن ده مينمعش انك بعد كده تروح تشوفهم  
<br>ركز علي ال mathematics عشان المجال مليان mathematics فيه linear mathematics and calculus  
<br>statistic and probability  
<br>نصيحة الدكتور هي انك لو عندك خلفية في الرياضيات ابدا اتعلم ال machine learning و انت ماشي شوف الي انت محتاجه من الرياضايات و روح اتعلمه ده هيقلل عليك من الحاجات الي المفروض انت تتعلمها في البداية  
<br>🟥after basics :  
<br>في الغالب ممكن تخش مجال ال deep learning و ده فيه مجالين هما : deep learning in computer vision or deep learning in NLP  
<br>database scientist : محتاج برده تعرف الي فاتو بالاضافة الي sql and database backend .  
<br>خلي بالك المجال مليان tools كتيرة عشان تتطلع بيها البيانات ؛ عشان كده انتبه ان الادوات دي ما هي الا ادوات و ليست غاية ، انما الغاية هو انك يكون عندك القدرة علي انك تتطلع ال insights او البيانات التي لها قيمه فعلا و مهمه يعني عايزه يكون عندك قوة ملاحظة  
<br>-🟩طب اعمل cv ازاي :  <br>
<br>لازم ال cv يكون فيه projects و مشاريع تكون كويسه تعرفه انك بتاع شغل مش جي تهزر ، يعني مثلا مشروع بتاع 5000 كود ولا حاجه ، و بعدين ارفع المشروع علي ال github و يكون فيه زي roadmap توضيحي بتفهم الي داخل يتفرج علي شغلك انت شغلك ماشي ازاي و او فيديوهات توضيحية ، خلي نفسك مكان الراجل الي هيتفرج عليك.  
<br>🟩فقرة الاسئلة :<br>
1.اعمل ايه عشان استثمر فترة الكلية ؟؟  <br>
<br>انك تروح تحضر و تشوف المواد ماشية ازاي  
<br>و كل مادة مهمة تروح تاخدلها كورسات بره الكلية ، و لكن ركز علي التقدير برده .  
<br>
<br>ايه افضل طريقة احسن بيها مهارة ال problem solving عندي ؟  
<br>
<br>افضل مسار انك تحدد roadmap و تمشي عليها زي مثلا الرود ماب بتاعة الدكتور : junior  
<br>
<br>ازاي احافظ علي الاستمرارية ؟  
<br>
<br>احب الاعمال ادومها و ان قل  
<br>و انك تمشي بمبدأ الترغيب و الترهيب انك تعرف و يكون حاضر في ذهنك انك لو ماستمرتش ايه الي هيحصل و لو استمريت ايه الي هيحصل .  
<br>انك تكون حاطط جدول لنفسك مثلا 5 ساعات problem solving في الاسبوع .  
<br>ادارة حالتك النفسية اثناء ال roadmap .  
<br>
<br>ازاي اختار التخصص ؟  
<br>
<br>اولا اعمل استكشاف لكل مجال من المجالات الي فاتت دي الاستكشاف مثلا لكل مجال 3 ساعات طب ايه هيا عناصر الاستكشاف :  
<br>بيشمل حجم السوق ، ايه نوعية الشغل ، هو انا بعمل ايه بالظبط  
<br>بعد كده تنتقل لمرحلة التجربه
]]></description><link>_zettlenotes\other-notes\الطريق-.md\الطريق-.md.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/الطريق .md</guid><dc:creator><![CDATA[مصطفي سعد]]></dc:creator><pubDate>Tue, 23 Jul 2024 16:39:35 GMT</pubDate></item><item><title><![CDATA[المسافة بين رغبتنا و انجازاتنا]]></title><description><![CDATA[ 
 <br>
<br>المشكله هنا ، انو الشخص مثلا يبقي عايز ابنه يطلع احسن منه و بعدين في فترة المراهقة يلاقي الولد و لا هو بار بوالديه و معندوش اساس في الدين و غير ذلك من الامثلة و هكذا من الامثلة  
<br>الحل هو :  
<br>
<br>يكون عندك وعي (بالاخذ بالاسباب) ايه الحاجات الي المفروض اعملها عشان مقعش هنا + يكون عندك معيارتقيس به تقدمك ، بس هنا فيه مشكله كبيره الا و هي ممكن الشخص ياخد بالاسباب الي هو اتعلمها من ابوه و امه و الي هي بالمناسبة مش شرط تكون صح و عدل عليها بعض التعديلات و برده محققش الهدف بتاعة ، هنا نقوله بقي اسأل نفسك انت الي اي درجة مؤهل انك تحط خارطه انا مفروض اعمل ايه عشان اوصل .  
<br>
<br>المشكله الاكبر بقي هي في التنفيذ  
<br>و برده فيه مشاكل الا و هي التشتت و خاصة في عصر الانترنت و وسائل التواصل ، لذلك اعمل في نفسك معروف و حاول تقلل استخدامك لهذه المستهلكات .  
<br>اكتب رغباتك ، و ليه اخترت الرغبات دي بالذات ، و ايه الي هيحصل لو عملت و لو معملتش ، و اكتب ايه المجهود اللازم و المعيار الي المفروض نمشي بيه .
]]></description><link>_zettlenotes\other-notes\المسافة-بين-رغبتنا-و-انجازاتنا.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/المسافة بين رغبتنا و انجازاتنا.md</guid><dc:creator><![CDATA[مصطفي سعد]]></dc:creator><pubDate>Tue, 23 Jul 2024 16:38:20 GMT</pubDate></item><item><title><![CDATA[الهشاشة النفسية]]></title><description><![CDATA[<a class="tag" href="?query=tag:ابن_القيم" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ابن_القيم</a> 
 <br><img alt="8f10544f88885a2da447489054b33ed7.jpg.webp > center" src="\lib\media\8f10544f88885a2da447489054b33ed7.jpg.webp"><br><br>اننا نقوم احياناُ بتضخيم اي مشكلة تظهر في حياتنا الي درجة تصويرها ككارثة وجودية . <br>نحن اليوم نعظم  مشاعرنا و نجلعها حكما نهائيا علي كل شيء تقريبا وم نقرر اعتزال كل ما يؤذي مشاعرنا و لو بكلمة بسيطة <br>"و قد اجمع عقلاء كل امة علي ان النعيم لا يدرك بالنعيم ، و ان من اثر الراحة فاتته الراحة ، و انه بحسب ركوب الاهوال و احتمال المشاق تكون الفرحة و اللذة فلا فرحة لمن لا هم له ، و لا لذة لمن لا صبر له ، و لا نعيم لمن لا شقاء له ، و لا راحة لمن لا تعل له ... بل اذا تعب العبد قليلاً استراح طويلاً ، و اذا تحمل مشقة الصبر ساعة قاده لحياة الأبد ، و كل ما فيه اهل النعيم المقيم فهو صبر ساعة ، و الله المستعان ولا قوة إلا بالله " - <a href=".?query=tag:ابن_القيم" class="tag" target="_blank" rel="noopener">#ابن_القيم</a> <br>يقسم الناس الي ثلاثة اقسام من حيث المرونة النفسية " ضعيف ،  وقوي ، متين غير قابل للكسر " و هذا راجع الي التربية التي تشأ عليها الشخص و مدي المرونة النفسية و القدرة علي المقاومة و الصبر علي المكاره . <br><br>LINKS TO THIS PAGE <br>Dataview: No results to show for list query.]]></description><link>_zettlenotes\other-notes\الهشاشة-النفسية.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/الهشاشة النفسية.md</guid><dc:creator><![CDATA[اسماعيل عرفة]]></dc:creator><pubDate>Mon, 29 Jul 2024 09:33:52 GMT</pubDate><enclosure url="lib\media\8f10544f88885a2da447489054b33ed7.jpg.webp" length="0" type="image/webp"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\8f10544f88885a2da447489054b33ed7.jpg.webp&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[انواع الجهل ]]></title><description><![CDATA[ 
 <br>
<br>علم لم يصل 
<br>ناتج عن علم وصل تفسيره خطأ 
<br>جهل ناتج عن غزارة العلم 
]]></description><link>_zettlenotes\other-notes\انواع-الجهل-.md\انواع-الجهل-.md.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/انواع الجهل .md</guid><dc:creator><![CDATA[الشيخ باسم]]></dc:creator><pubDate>Fri, 26 Jul 2024 22:30:44 GMT</pubDate></item><item><title><![CDATA[انواع المعرفة]]></title><description><![CDATA[ 
 <br>
<br>معرفة توسع المدارك عند الانسان ( تغير من طريقة تفكيره )
<br>معرفة تقتدي خطوات عملية للوصول الي النتيجة المرغوب فيها  
]]></description><link>_zettlenotes\other-notes\انواع-المعرفة.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/انواع المعرفة.md</guid><dc:creator><![CDATA[علي محمد علي]]></dc:creator><pubDate>Fri, 26 Jul 2024 22:28:49 GMT</pubDate></item><item><title><![CDATA[اهداف ام عادات]]></title><description><![CDATA[ 
 <br>كيف تختار الأنسب ما بين الحلين ( عادات ام اهداف ) ؟  <br>
<br>إذا كان الشيء الذي تسعى إلى تحقيقه المفروض تهتم بيه لفتره طويله فيفضل يكون عادة خيارك الأساسي و ليس الوحيد عشان ممكن يكون في العادة هدف تريد تحقيقه خلال فتره زمنية قصيرة.  
<br>إذا كان الهدف محدد بفتره زمنيه
]]></description><link>_zettlenotes\other-notes\اهداف-ام-عادات.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/اهداف ام عادات.md</guid><dc:creator><![CDATA[علي محمد علي]]></dc:creator><pubDate>Tue, 23 Jul 2024 16:34:12 GMT</pubDate></item><item><title><![CDATA[اهم الصفات في المبرمج الناجح]]></title><description><![CDATA[ 
 <br>
<br>الرغبة في حل المشاكل و انك تكون ((حلال مشاكل ))  
<br>
<br>لازم تعرف تعمل debugging و تكون فاهم الكود بتاعك ماشي ازاي و تعرف تعمل key point  
<br>لازم تكون مرتاح و انت بتقرأ باللغة الانجليزية ( علي الاقل قراءة ) ، عشان انت كل شوية هتروح تقرأ documentations و تدور علي حلول و بتبقي في الاغلب باللغة الانجلزية .  
<br>
<br>الفضول المستمر ، لازم تكون مطلع و تواكب العصر بما له علاقة بالتكنولوجيا  
<br>
<br>يعني كل فتره تقرأ مقالة او تسمع بودكاست تشوف الدنيا ماشية ازاي  
<br>
<br>تكون معلم كفؤ : الرغبة في تدريس المادة للآخرين  
<br>المعرفة الشاملة ( الصورة الكاملة للشيء الي بيتشغلوا عليه )  
<br>.التأثير الايجابي علي البيئة الي حواليك
]]></description><link>_zettlenotes\other-notes\اهم-الصفات-في-المبرمج-الناجح.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/اهم الصفات في المبرمج الناجح.md</guid><dc:creator><![CDATA[]]></dc:creator><pubDate>Mon, 29 Jul 2024 01:35:38 GMT</pubDate></item><item><title><![CDATA[اول 20 ساعة ]]></title><description><![CDATA[<a class="tag" href="?query=tag:اضافات_خاصة_بي" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#اضافات_خاصة_بي</a> 
 <br>
<br>اختر مهارة محببة لك او تثير فضولك 
<br>ركز علي مهارة واحدة فقط 
<br>حدد المستوي المستهدف قبل البدء 
<br>فكك المهارة الي مهارات اصغر 
<br>جهز الادوات اللازمة مسبقاً 
<br>تخلص من الموانع و المشتتات 
<br>ضع وقت محدد للتدريب 
<br>تدرب بالساعة لفترات قصيرة 
<br>اجعل لتدريبك ردود فعل سريعة 
<br>ركز علي الكم و السرعة و ليس الاتقان في البداية<br>
( و عندما تدرك ان هذا المجال هو الذي يناسبك حقاً استدرك الامور الاخري التي تساعدك علي ادراك المادة و ابدأ في التعمق فيها ) <a href=".?query=tag:اضافات_خاصة_بي" class="tag" target="_blank" rel="noopener">#اضافات_خاصة_بي</a> 
]]></description><link>_zettlenotes\other-notes\اول-20-ساعة-.md\اول-20-ساعة-.md.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/اول 20 ساعة .md</guid><dc:creator><![CDATA[علي محمد علي]]></dc:creator><pubDate>Thu, 25 Jul 2024 06:02:41 GMT</pubDate></item><item><title><![CDATA[تحمل المسؤولية ]]></title><description><![CDATA[ 
 <br>
<br>المشكلة الكبيره الا و هي انك بتنتقل من صفر مسؤولية الي over مسؤولية ، الحل هو يجب التدرج .  
<br>المسؤوليات تنقسم الي جزئين هما : مسؤولية شخصية و اخري مجتمعية .  
<br>اعظم مسؤولية للانسان تجاه نفسه انه يسق طريقة الي الله عزوجل .  
<br>و من المسؤوليات المجتمعيه الا و هي الامر بالمعروف و النهي عن المنكر .  
<br>المسؤولية قرار و لها تابعات فلازم تتحمل عواقب هذا القرار و لا ترمي الاحمال و الغلطات علي غيرك .  
<br>فيه مشكله الا و هي ان الانسان بقي يربط الحاجات الي في حياته علي حسب مزاجه مش علي حسب المسؤولية دي مشكله كبيره جدا ، لذلك عود نفسك علي العمل علي الحاجة سواء ليك مزاج ليها او ملكش مزاج .
]]></description><link>_zettlenotes\other-notes\تحمل-المسؤولية-.md\تحمل-المسؤولية-.md.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/تحمل المسؤولية .md</guid><dc:creator><![CDATA[مصطفي سعد]]></dc:creator><pubDate>Tue, 23 Jul 2024 16:37:16 GMT</pubDate></item><item><title><![CDATA[تعريف الحرية ]]></title><description><![CDATA[ 
 <br>تعريف الحرية هي عبادة الله فوق ما سواه]]></description><link>_zettlenotes\other-notes\تعريف-الحرية-.md\تعريف-الحرية-.md.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/تعريف الحرية .md</guid><dc:creator><![CDATA[عبد الرحمن ذاكر الهاشمي]]></dc:creator><pubDate>Tue, 23 Jul 2024 16:34:55 GMT</pubDate></item><item><title><![CDATA[تكوين النفس]]></title><description><![CDATA[ 
 <br>فطرة ( عامة و خاصة )<br>
كسب ( بسعي و بغير سعي )<br>
علم و عمل<br>
مدخلات ( حس تفكير عقل )<br>
عمليات<br>
مخرجات ( فكرة شعور سلوك )]]></description><link>_zettlenotes\other-notes\تكوين-النفس.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/تكوين النفس.md</guid><pubDate>Tue, 23 Jul 2024 14:44:07 GMT</pubDate></item><item><title><![CDATA[دورة التفكير المنطقي]]></title><description><![CDATA[ 
 <br>ما هو العدل ؟ العدل ليس المساواة و لكن اعطاء كل ذي حق حقه <br><br>
<br>الادراك الحسي 
<br>الادراك الخيالي 
<br>الادراك العقلي ( فهم المعاني الاسسية زي العدل و الحرية و الحق )<br>
و كل ما سبق يندرج تحت مصطلح العلم الحصولي ، اما العلم الحضوري هو علم الانسان بنفسه ( الشعور بالحزن او الاكتئاب و هكذا ... ) 
<br><br>
<br>هو القوانين الطبيعية للتفكير و قائم علي البديهيات 
<br>المنطق يدرس التفكير 
<br>العلم الباحث في قواعد التفكير 
<br>العقل يدرك المعاني العامة او الكلية 
<br>
]]></description><link>_zettlenotes\other-notes\دورة-التفكير-المنطقي.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/دورة التفكير المنطقي.md</guid><pubDate>Fri, 26 Jul 2024 00:21:03 GMT</pubDate></item><item><title><![CDATA[صوتية الصحة]]></title><description><![CDATA[ 
 <br>عند الدخول في اي مجال او لو أنت مهتم بإكتساب عادة جيدة ، فعليك بسؤالين مهمين يعيناك علي الاستمرارية و المداومة الا و هما ( ماذا و لماذا  ) . <br><br>ترويض الشيء اي تمرينة علي عمل شيء معين .<br>
و الرياضة نوعان : <br>
<br>رياضة البدن 
<br>رياضة القلب / الروح 
<br><br>
<br>و هو تزليل / اخضاع النفس ( الدابة ) علي فعل شيء معين
]]></description><link>_zettlenotes\other-notes\صوتية-الصحة.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/صوتية الصحة.md</guid><dc:creator><![CDATA[لطفي احمد الطوخي]]></dc:creator><pubDate>Tue, 23 Jul 2024 19:25:27 GMT</pubDate></item><item><title><![CDATA[طريقة تحصيل المعرفة و تعلمها و حفظها]]></title><description><![CDATA[ 
 <br>
<br>جمع المعرفة أو المتشابه من العلوم أو التي تتحدث عن نفس الموضوع في مكان واحد ؛ لكي يسهل مراجعتها فيما بعد و لكي نكون ذاكره تصويرية عن المعلومه أو المعرفة .  
<br>مراجعه ما تم تحصيله علي فترات متباعدة لكي تثبت أكثر في الدماغ . 
]]></description><link>_zettlenotes\other-notes\طريقة-تحصيل-المعرفة-و-تعلمها-و-حفظها.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/طريقة تحصيل المعرفة و تعلمها و حفظها.md</guid><dc:creator><![CDATA[صالح العصيمي]]></dc:creator><pubDate>Tue, 23 Jul 2024 16:13:54 GMT</pubDate></item><item><title><![CDATA[طريقتي بالتفصيل في المذاكرة و حل الامتحان ]]></title><description><![CDATA[ 
 <br>
<br>اقرأ الكتاب مره واحده و لخصه بطريقتك في الكشكول 
<br>متحاولش ترهق نفسك بالحفظ 
<br>حاول تحسب الحاجة الي بتعملها 
<br>وقت المذاكرة حد 
<br>افصل (Pomodoro )
]]></description><link>_zettlenotes\other-notes\طريقتي-بالتفصيل-في-المذاكرة-و-حل-الامتحان-.md\طريقتي-بالتفصيل-في-المذاكرة-و-حل-الامتحان-.md.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/طريقتي بالتفصيل في المذاكرة و حل الامتحان .md</guid><dc:creator><![CDATA[الاحسان الشقيفي]]></dc:creator><pubDate>Fri, 26 Jul 2024 16:22:45 GMT</pubDate></item><item><title><![CDATA[عن الاحباط و قلة الدافع ]]></title><description><![CDATA[<a class="tag" href="?query=tag:خالد_ابو_شادي" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#خالد_ابو_شادي</a> 
 <br>من الاسباب الرئيسية و من اكبر اضرار و مؤثرات السوشيال ميديا هي : استعجال النتيجة<br>
و من اكثر الاشياء التي يستعجل الناس اثناء عملها هي ( الدايت  - تعلم لغة انجليزية او اجنبيه ) بسرعة<br>
فمن اولي الحلول هي الانقطاع عن السوشيال ميديا التي تسبب لك هذا الاحباط<br>
ركز علي العمل و ليس النتيجة .. احسان العمل و الخطوات<br>
قال تعالي : " و ان ليس للانسان الا ما سعي و ان سعيه سوف يري "<br>
ركز علي العمل اليومي بإحسان ..<br>
" الجودة متناسبة عكسيا مع السرعة  " - <a href=".?query=tag:خالد_ابو_شادي" class="tag" target="_blank" rel="noopener">#خالد_ابو_شادي</a><br>
"فيه فرق بين انك تنجز شيء بسرعة لانه بسيط و بين ان حياتك كلها تمشي برتم سريع " <br>الاحباط و قلة العمل مرتبطة بحاجتين : المقارنة و عدم اشغالك بوردك اليومي من الاعمال و تجنب السوشيال ميديا ]]></description><link>_zettlenotes\other-notes\عن-الاحباط-و-قلة-الدافع-.md\عن-الاحباط-و-قلة-الدافع-.md.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/عن الاحباط و قلة الدافع .md</guid><dc:creator><![CDATA[قناة نافع]]></dc:creator><pubDate>Fri, 26 Jul 2024 22:43:00 GMT</pubDate></item><item><title><![CDATA[عوامل النجاح ( مثلث النجاح )]]></title><description><![CDATA[ 
 <br>
<br>العمل صالح لا يغضب الله عز و جل  
<br>السعي وراء الشيء  
<br>ان يكون مناسب لقدراتك في الوقت الحالي
]]></description><link>_zettlenotes\other-notes\عوامل-النجاح-(-مثلث-النجاح-).html</link><guid isPermaLink="false">_ZettleNotes/other Notes/عوامل النجاح ( مثلث النجاح ).md</guid><dc:creator><![CDATA[علي محمد علي]]></dc:creator><pubDate>Tue, 23 Jul 2024 14:43:20 GMT</pubDate></item><item><title><![CDATA[فقه اليوم و الليلة ]]></title><description><![CDATA[ 
 <br>
<br>الورد اليومي (قراءة صفحة من القرآن مع تفسيرها +سنة الفجر + صلاة الضحي + اذكار الصباح + اذكار المساء ) 
<br>الصلاة علي وقتها 
<br>قراءة عامة ( الله ثم انا و الاخرين ) 
<br>قراءة خاصة ( فقه الاستخلاف ) 
<br>الرياضة و الصحة 
<br><br>و عليك بـ  : <br>
<br>قراءة النفس ( معرفة ما تريد و كيفية تهذيبها ) 
<br>قبول النفس ( بعد ان عرفتها عليك ان تتقبلها كما هي مع العزم و السعي علي الاصلاح منها ما استطعت من قوة ) ويأتي ذلك بالعلم و العمل 
<br>تزكية النفس ( من علم استخلاف و ترويض النفس بدنيا و جسديا ) 
]]></description><link>_zettlenotes\other-notes\فقه-اليوم-و-الليلة-.md\فقه-اليوم-و-الليلة-.md.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/فقه اليوم و الليلة .md</guid><dc:creator><![CDATA[عبد الرحمن ذاكر الهاشمي]]></dc:creator><pubDate>Fri, 26 Jul 2024 22:35:58 GMT</pubDate></item><item><title><![CDATA[كتاب عام ال 12 اسبوع]]></title><description><![CDATA[ 
 <br>🟢12 اسبوع بدلا من 12 شهر :<br>
🔴عيوب تخطيط فتره زمنية طويلة  <br>
<br>يدفع الي التراخي و التسويف  
<br>يًغري البعض يوضع عدد كبير من الأهداف  
<br>دقة التخطيط تكون أقل.<br>
🔴مميزات التخطيط لفترة زمنية قصيرة  
<br>يدفعك للعمل علي أهدافك و تقلل من التراخي و التسويف .  
<br>تجبرك علي التركيز علي عدد قليل من الأهداف و هذا عادة يحقق نتائج افضل.  
<br>🟢كتابة الخطة  <br>
<br>عشان تقدر تعمل خطة للـ12 اسبوع محتاج تعرف الـ [الرؤية - الأهداف - التكتيكات ]<br>
🔴الرؤية : هي التصور الواضح للمستقبل المرغوب ،يفضل تكون من سنة لخمس سنوات<br>
🔴الأهداف فقط لفتره الـ 12 اسبوع : اتباع طريقة ال SMART<br>
🔴التكتيكات : هي افعال محدد تساعدك علي تحقيق الهدف<br>

<br>🟢الاسبوع الـ 13 :  <br>
<br>الايجابيات  
<br>السلبيات  
<br>الدروس المتسفادة  
<br>التخطيط للـ 12 الاسبوع القادم
]]></description><link>_zettlenotes\other-notes\كتاب-عام-ال-12-اسبوع.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/كتاب عام ال 12 اسبوع.md</guid><dc:creator><![CDATA[علي محمد علي]]></dc:creator><pubDate>Thu, 25 Jul 2024 06:02:34 GMT</pubDate></item><item><title><![CDATA[كتب لمصطفي محمود]]></title><description><![CDATA[ 
 <br>
<br>الشيطان يحكم .. مجموعة مقالات مجمعة و يتحدث عن سبب الوجود و بعض الارقام المتعلقه بالقران الكريم كالرقم 7  
<br>عصر القرود .... شهوة الإنسان و فكرة الاباحيه  
<br>لغز الموت  
<br>الوجود و العدم .... صعوبة تغيير العواطف  
<br>رحلتي من الشك الي الإيمان ... يفضل البدأ بها  
<br>حوار مع صديقي الملحد  
<br>اللذين ضحكوا حتي البكاء ..رواية  
<br>اسرائيل البداية و النهاية  
<br>اينشتاين و النسبيه  
<br>لماذا رفضت الماركسية  
<br>رواية العنكبوت
]]></description><link>_zettlenotes\other-notes\كتب-لمصطفي-محمود.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/كتب لمصطفي محمود.md</guid><pubDate>Tue, 23 Jul 2024 16:07:43 GMT</pubDate></item><item><title><![CDATA[كي لا تقع في فخ الكبر ]]></title><description><![CDATA[ 
 <br>
<br>من ال shorts الي فادتني و صلحت عندي مفاهيم كتيره عن الكبر و ما هو .... بكل بساطه اي موقف قمت بـ بطر الحق و غمط الناس فهو كبر ما دون ذلك فهو غير ذلك  
]]></description><link>_zettlenotes\other-notes\كي-لا-تقع-في-فخ-الكبر-.md\كي-لا-تقع-في-فخ-الكبر-.md.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/كي لا تقع في فخ الكبر .md</guid><dc:creator><![CDATA[عبد الرحمن ذاكر الهاشمي]]></dc:creator><pubDate>Tue, 23 Jul 2024 16:30:39 GMT</pubDate></item><item><title><![CDATA[كيف تفشل في كل شيء ثم تحقق النجاح الكبير ]]></title><description><![CDATA[ 
 <br>
<br>الفشل المقصود : الفشل الي بيستفيد منه بمعرفة او خبرة او مهارة 
<br>
<br>حاول تعمل جلسة مع نفسك او قرايبك لمعرفة اسباب الفشل 
<br>كمان ممكن تتعلم من فشل الآخرين 
<br>الاهداف للفشلة و الانظمة للناجحين<br>
البديل من الاهداف هو الانظمة : وضع قواعد و عادات يومية مثال : بدلاُ من تحديد هدف ان تقل زونك لكي يصل الي 60 كيلو خلال سنة و تبقي حاطط كل تركيزك توصل للهدف ده ، الافضل انك تحط نظام دائم للحياة الصحية من اكل و رياضة  تستمر عليه بشكل دائم . 
<br>ازاي نوفق بين كلام المؤلف الي جزء منه صحيح و بين عشرات الكتب الي بتتبني اهمية وجود الاهداف في حياتنا ؟؟ 

<br>اجعل الاهداف جزء من خطة استراتيجية كبيرة 
<br>حول اهدافك لعادات يومية و احتفل بالنجاحات الصغيرة 
<br>راجع اهدافك كل فترة و غير فيها ما يلزم 


<br>ازاي تضاعف فرصك في النجاح 

<br>كل مهارة تكتسبها .. تضاعف فرصك في النجاح . 
<br>يكفي ان تكون مقبول في المهارات . (بشكل عام الا فقه الاستخلاف فيفضل ان تكون متقن المهارة التي انت مستخلف فيها امام الله تعالي ) 
<br>كرر اكثر من مرة 
<br>كن متفائل 


]]></description><link>_zettlenotes\other-notes\كيف-تفشل-في-كل-شيء-ثم-تحقق-النجاح-الكبير-.md\كيف-تفشل-في-كل-شيء-ثم-تحقق-النجاح-الكبير-.md.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/كيف تفشل في كل شيء ثم تحقق النجاح الكبير .md</guid><dc:creator><![CDATA[علي محمد علي]]></dc:creator><pubDate>Fri, 26 Jul 2024 15:48:02 GMT</pubDate></item><item><title><![CDATA[كيف تنجز في هذا العالم المختل]]></title><description><![CDATA[ 
 <br>امرين رئيسيين يساعدانك علي الانجاز هما :  <br>
<br>الانضباط  
<br>
<br>اكتب السبب الدافع لعمل الشيء الي انت عايز تعمله ( لازم تعرف ليه تعمل الشيء ده )  
<br>حاول تشيل المشاعر و المزاج من المعادلة  
<br>خد خطوات بسيطه و متدرجة  
<br>المراجعة و التقييم  
<br>
<br>التركيز 
]]></description><link>_zettlenotes\other-notes\كيف-تنجز-في-هذا-العالم-المختل.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/كيف تنجز في هذا العالم المختل.md</guid><dc:creator><![CDATA[احمد ابو زيد]]></dc:creator><pubDate>Thu, 25 Jul 2024 22:42:12 GMT</pubDate></item><item><title><![CDATA[لايف مع لطفي احمد الطوخي ]]></title><description><![CDATA[ 
 <br>"ما دخل وقته فهو أولى "حسين عبدالرازق  <br>كيف اخطط أهدافي و خارطه عامة لوضع الاهداف و التخطيط مقطع لطفي احمد الطوخي  <br>منصات التواصل الاجتماعي تضع في دماغك فكره المقارنة بشكل لا واعي ... مين الي قالك الي انت بتعمله مش كافي  <br>اي مجال هتتقي الله فيه و تتعلمه هيبقي مجني  <br>لا تزتري انجازك و إن قل  <br>بقدر وضوح خطك بقدر ثباتك و انك تكون راسي  <br>تعلم مهارات التعلم و التلخيص و التدوين ...تحسن جودة التعلم..آليات التعلم الفعال "تيلجرام"..  <br>الاتقان : أن مفيش حاجه تقف قدامي في مسأله معينه في المستوي الي انا فيه  <br>عمل تلخيص المحاضره أو درس ورقتين و لا حاجه عشان لما ارجع ليها تاني يكون معايا الملخص ده و مرجعش ابذل نفس الوقت الي بذلته في الاول  <br>• اعرف منين احساس بين المجال مش مناسب ليا و لا انا معنديش صبر ؟  <br>
<br>اختبر نفسك في المجال ده و اتقي الله فيه الاول ..لا تعتمد علي المشاعر و الاحاسيس لأنها متقلبه  
<br>الصعاب تزال بعدين  
<br>التعلم مش تدريجي..منحني exponential  
<br>بعد الكورس كامل ارجع اشوف الجزء العملي بعد إنهاء الكورس ، و ارجع اشوف صاحب الكورس  
<br>في أجزاء بتمشي مع بعض زي جانب الصحه و الشغل  
<br>الفرصه تأتي للي جاهز  
<br>اي احساس ينتابني لازم اتاكد منه  <br>الحفاظ علي الصحه النفسية و التزكوية يكون له الاولويه  <br>اسئلة<br>
عملية الأرشفة المواد التي تذاكرها ، و هل اتعلم كورس اوبسيديان ؟<br>
بدرس فهل اروح اشتغل دلوقتي ؟]]></description><link>_zettlenotes\other-notes\لايف-مع-لطفي-احمد-الطوخي-.md\لايف-مع-لطفي-احمد-الطوخي-.md.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/لايف مع لطفي احمد الطوخي .md</guid><dc:creator><![CDATA[لطفي احمد الطوخي]]></dc:creator><pubDate>Tue, 23 Jul 2024 16:42:27 GMT</pubDate></item><item><title><![CDATA[لمن أراد الفهم..هدم فكر منظمة تكوين بمحاضرة]]></title><description><![CDATA[ 
 <br>من علامات اكتشاف العلوم الزائفة ( التي تؤدي بالتشويش العام في المجتمعات ) :  <br>
<br>ان الشخص يقوم بنشر كلامه في اي علم من العلوم في الصحافة و التلفاز و ليس في المؤتمرات العلمية او اماكن البحث المعتبرة فهو كذاب بنسبة 99.9 % لانه لو كان صادق كان اتخذ الاماكن المتخصصه في هذا الامر و اصبح لديه براءة اختراع او استطاع ان يأخذ دخل جيد بسبب اختراعه او اكتشافه هذا .  
<br>معظم استدلالته حاجه شفهية  
<br>لا يوجد ابحاث مكمله لبحثه  
<br>ولازم عشان تصدقة لازم تهدله خمس ست قوانين من المستقره في علم من العلوم  
<br>اللجوء للمجال العام و ليس الجهات المتخصصه<br>
الشك المنهجي ( 50 50 ) و بيحاول يوصل لاي حاجه تزود احد الطرفين<br>
المؤمن المدهش ( يعيش في مجتمع كله ملحدين و تلاقيه اسلم )<br>
الملحد المدهش ( يعيش في مجتمع كله مؤمنين و تلاقيه الحد )<br>
الشك الصفصطائي ( الملحد المراهق ) : حرية تامه ( عبادة الهوي ) ، كل شوية يديك شبهات و جدل و مش عايز يسمع الرد علي الشبهات  
<br>و السبب الاساسي للوصول الي هذه المشكلة ( حالة الاستلحاد ) هو عدم الجدية العامة ( و التحدث مع الامور الكبري بهذا الاستخفاف )  <br>طب ايه الحل ؟<br>
التعامل الجاد ، الوقوف علي ارضية ثابتة ( البحث عن الادوات المنهجية ) منهج علمي]]></description><link>_zettlenotes\other-notes\لمن-أراد-الفهم..هدم-فكر-منظمة-تكوين-بمحاضرة.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/لمن أراد الفهم..هدم فكر منظمة تكوين بمحاضرة.md</guid><dc:creator><![CDATA[ايمن عبد الرحيم,Andrew ng]]></dc:creator><pubDate>Tue, 23 Jul 2024 14:41:45 GMT</pubDate></item><item><title><![CDATA[مفهوم الرضاء]]></title><description><![CDATA[ 
 <br>قبول الحال التي انت عليها الان ، و ليس الاكتفاء بها]]></description><link>_zettlenotes\other-notes\مفهوم-الرضاء.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/مفهوم الرضاء.md</guid><dc:creator><![CDATA[علي محمد علي]]></dc:creator><pubDate>Tue, 23 Jul 2024 16:14:27 GMT</pubDate></item><item><title><![CDATA[من انا ؟ و لما انا ؟ لماذا خلقت]]></title><description><![CDATA[<a class="tag" href="?query=tag:عبد_الرحمن_ذاكر_الهاشمي" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#عبد_الرحمن_ذاكر_الهاشمي</a> <a class="tag" href="?query=tag:عبد_الرحمن_ذاكر_الهاشمي" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#عبد_الرحمن_ذاكر_الهاشمي</a> 
 <br><br>
<br>يمكن ان تأخذ او تتأثر بذلك و تلك و لكن يجب ان يمروا في نهاية المطاف علي فلترك الخاص .
<br><br>
<br>الهوية : هو ما يميز الذات الانسانية عن غيرها . 
<br>الوظيفة : هي محل ما تفعله النفس / ما تتبناه النفس يترتب عليه حقوق و واجبات سعياً للوصول الي الهدف ( الوظيفة لا تساوي المهنة بل المهنة جزء من الوظيفة ) . 
<br>الهدف : القصد الذي أراه في نهاية الطريق و اسعي له حقيقة ( اخر رؤيتي ) . 
<br><br>
<br>الهدف الاسمي : هو الذي لا اجد بعدها جوابا علي سؤال " ثم ماذا ؟ " . 
<br><br>"الهوية تتحدد من خلال الاهدافي " - <a href=".?query=tag:عبد_الرحمن_ذاكر_الهاشمي" class="tag" target="_blank" rel="noopener">#عبد_الرحمن_ذاكر_الهاشمي</a><br>
<img alt="Pasted image 20240723200643.png" src="\lib\media\pasted-image-20240723200643.png" style="width: 300px; max-width: 100%;"><br><br>
<br>هناك مرجعية اكيد لتحديد الهدف 
<br>
<br>ان يكون الهدف من تحديدي انا 
<br>التأكد من ارادتي لهذا الهدف ، كيفية التحقق : ماذا عن جدولي اليومي ، ما الذي يشغل جدولي اليومي 
<br>ان اتأكد ان هذا الهدف يحمل قيمة ايجابية معه (انزل شوف تفاصيل اكتر عن هذا الهدف من نفع يعود علي نفسي و الأمه الاسلامية و هل يستحق هذا العناء الذي يبذل )<br>
" كلما انشغلت بهدفي الكبير اكثر كلما سهل علي السعي في الطريق  " - <a href=".?query=tag:عبد_الرحمن_ذاكر_الهاشمي" class="tag" target="_blank" rel="noopener">#عبد_الرحمن_ذاكر_الهاشمي</a> 
<br><br>
<br>تقسيم اليوم / و عمل جدول يومي 
<br><br>" و ما خلقت الانس و الجن الا ليعبدون "<br>
علينا ان ننطلق من القرآن الكريم و السيرة النبوية الصحيحة .<br>
الترويح جانب مهم <br><br>اقل القليل ولا تفتح علي نفسك جبهات كثيرة <br>
<br>الصلاة علي وقتها 
<br>سنة فجر 
<br>ركعتي ضحي 
<br>ورد يومي ركعة الوتر 
<br>تعلم مادة شرعية 
<br>تعلم ما انا مستخلف فيه الان 
<br>راحة الجسم
<br>الترويح المباح 
<br><br>طالما اجتمعت لدي المعلومة بوضوح و جلاء ، و اسلوب و قدرة علي التعبير عنها دون ان اظلم المعلومة و دون ان اسيء هذه هي المرحلة ( وضوح المعلومة في الذهن + اسلوب البيان ) . <br><br>
<br>قراءة القرآن الكريم و تفسير ميسر بسيط 
<br>قراءة الناس الذين نظروا للأمور بمنظار عاقل بعيداً عن المشاعر أمثال  ( محمد قطب سيد قطب ، علي عزت بيتوفينشي ، عبد الوهاب المسيري ، فريد الانصاري )  
<br><br>
<br>العفوية ليست مشكلة تحتاج الي حل ، بل نعمة من الله عزوجل و لكن أن تكون بشيء من الفطنة و الحكمة ... ان يلعم الانسان كيف يكون عفوياً دون احداث ضرر ، العفوية لا تعني السذاجة او قول ما لا ينبغي ان يُقال او ادخل نفسي في مشكلات هذه لم تعد عفوية بل اصبحت ( سفه ) و من الامور التي تساعد علي العفوية الصحية السليمة هو قراءة الكتب التي تتحدث عن تصرف الصحابة و أساليب الدعوة . 
<br><br>
<br>تذكر ان الله هو الذي يؤتي العبد و يتفضل عليه سبحانه ، و يجب ان يطلب كي يحصل عليه . 
<br>قراءة كتب العبودية لابن تيمية ، و المنقذ من الضلال للغزالي لتعرف اكثر عن الله <a data-href="من انا ؟ و لما انا ؟ لماذا خلقت#^ab0750" href="\_zettlenotes\other-notes\من-انا-؟-و-لما-انا-؟-لماذا-خلقت.html#^ab0750" class="internal-link" target="_self" rel="noopener">من انا ؟ و لما انا ؟ لماذا خلقت &gt; ^ab0750</a>
<br>قراءة كتاب عقيدة المسلم للامام الغزالي و جدد حياتك <a data-href="من انا ؟ و لما انا ؟ لماذا خلقت#^94b467" href="\_zettlenotes\other-notes\من-انا-؟-و-لما-انا-؟-لماذا-خلقت.html#^94b467" class="internal-link" target="_self" rel="noopener">من انا ؟ و لما انا ؟ لماذا خلقت &gt; ^94b467</a>
]]></description><link>_zettlenotes\other-notes\من-انا-؟-و-لما-انا-؟-لماذا-خلقت.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/من انا ؟ و لما انا ؟ لماذا خلقت.md</guid><dc:creator><![CDATA[عبد الرحمن ذاكر الهاشمي]]></dc:creator><pubDate>Thu, 25 Jul 2024 22:00:17 GMT</pubDate><enclosure url="lib\media\pasted-image-20240723200643.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240723200643.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[مهارة التدوين و التلخيص و الارشفة]]></title><description><![CDATA[<a class="tag" href="?query=tag:لطفي_احمد_الطوخي" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#لطفي_احمد_الطوخي</a> 
 <br><br>
<br>مقدمات هامة . 
<br>ثمرات التدوين و التلخيص و الارشفة 
<br>من التدوين الي الارشفة 
<br>ومضات و نصائح عامة 
<br><br><br>
<br>ماهية التدوين : الكتاب الورقية ، لما لها من فوائد علي الحفظ و جودة المذاكرة ، و ذلك لأنة يجعل الجسم كله يتفاعل مع المادة و زيادة عدد الوصلات العصبية النشطة و زيادة معدل التركيز . 
<br>ما هي الفائدة : هي اي معلومة جديدة عليك و تشعر انها مهمة ، او المعلومات المهمة حتي و لو مررت عليها سابقا . 
<br>ما المقصود بالتلخيص : كتابة الافكار الرئيسية للمادة و ما يسهل فهمها و الامثلة التي توضح الهم منها بأقل عبارة ممكنة . 
<br>ما هي الارشفة : هي عملية تصنيف و تخزين الفوائد و الملخصات في اماكن تسهل علينا الرجوع لها مستقبلاً 
<br><br>
<br>تعميق فهم المادة 
<br>الحفاظ علي الوقت و عدم إهدار الوقت المبذول في التعلم . 
<br> ؟ ليه الخص بالرغم من أن التلخيص هيستهلك مني وقت اضافي ؟<br>
<br>فرق بين المشاهدة و التعلم ، لان الي بيتعلم بيحتاج يرجع الي المادة الي لخصها مرة اخري و عملية التلخيص بتعمق فهم المادة 
<br>إن احتاج الي الرجوع للمادة دي مرة اخري هيرجع للملخص الي ثم كتابتة مرة اخري و في وقت اقل بكتير 
<br><br><img style="max-width:500px; " class="excalidraw-svg excalidraw-embedded-img excalidraw-canvas-immersive" src="blob:\\cc75b20c-2de8-4d3f-8d55-de2e285527c2" filesource="Excalidraw/شكل الصفحة.md" w="500" draggable="false" oncanvas="false"><br><br>
<br>استخدم عدة الوان في الكتابة و لكن فيه بينه و اضحة ثابتة للكتابة مثال " استخدام الالوان ( الازرق و الاسود و  الاخضر ) "
<br>اذا كتبت اقتباس خط علامتي تنصيص و قول اسم صاحب الاقتباس 
<br>و اكتب بطريقتك انت و لا تنقل ما يقوله بالحرف " التفريغ " إحنا مش بنفرغ ، و ده بيحسن عملية التعلم . 
<br><br>
<br>الكتابة الاولي : المصاحبة لأول مرة في تلقي المعلومة و قد يكون فيها كلام كثير اقدر اخليه مختصر ( الكتابة او التدوين المهارة ستتحسن مع الوقت ) 
<br>التلخيص : فلترة و تنقيح ما كتبته في الكتابة الاولي بحيث تأخذ اجوده و فيده بالنسبة لك 
<br>امتي بقي الخص ؟؟ : يفضل ترك مدة بين الكتابة الاولي و التلخيص "3 او 4 ايام " يعني مثلاً خلي 5 ايام في الاسبوع للتعلم و اخر يومين للمراجعة و التلخيص .<br>
؟ طب ليه ؟
<br>
<br>عشان كده اكنك بتراجع علي المادة 
<br>عشان تبقي طبقت التكرار المتباعد مما يساعد علي ثبات المعلومة اكثر في الدماغ . 
<br>
<br>الارشفة
<br><br>
<br>المواد انواع و ليست كلها علي نفس القدر من الاهمية : يعني فيه فيديو " ركيك " مينفعش انا اصلاُ اتفرج عليه ، و فيه فيديو مفيد و لكن لا يستحق ادون فيه الفوائد و بالتالي ليست كل المواد تلخص . 
<br>المواد نفسها انواع : 1. التزكوية التي الهدف منها حالة قلبية اكثر من كونها تحصيل معرفي
<br>ماذا عن الكتابة الرقمية ؟؟ لكل مقام مقال - <a href=".?query=tag:لطفي_احمد_الطوخي" class="tag" target="_blank" rel="noopener">#لطفي_احمد_الطوخي</a> منقول عن ؟؟؟ عشان كده لازم تعرف إيه الهدف من التلخيص و الاكمل الجمع بين الطريقيتين بحيث يكون الكتابة الاولية علي الورق و كذلك التلخيص ثم بعد مدة زمنية " شهر او شهرين" اراجع الملخص ده من خلال اني انقله نقلاُ كتابته و ايوه هياخد منك و قت و  لكن هكذا التعلم .  
]]></description><link>_zettlenotes\other-notes\مهارة-التدوين-و-التلخيص-و-الارشفة.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/مهارة التدوين و التلخيص و الارشفة.md</guid><dc:creator><![CDATA[لطفي احمد الطوخي]]></dc:creator><pubDate>Thu, 01 Aug 2024 15:33:35 GMT</pubDate><enclosure url="blob:app:\\obsidian.md\cc75b20c-2de8-4d3f-8d55-de2e285527c2" length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;blob:app:\\obsidian.md\cc75b20c-2de8-4d3f-8d55-de2e285527c2&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[نصائح وتوصيات هامة]]></title><description><![CDATA[ 
 <br>
<br>لا تتعلموا فقط " اعملوا " ... الزموا التزكية علما و عملا  
<br>اختبر نفسك في مواطن الخصومة ، في مواطن الخلاف و ما ليس علي الهوي  
<br>اكثروا من الصمت ( ينصح كتاب الصمت لابن ابي الدنيا )  
<br>ارفعوا سقف المعقولات .. ( سيبقي هناك الخلاف )  
<br>ابعد نفسك عن مواطن الفتنه و تشغل الناس بطاولتك الخاصة
]]></description><link>_zettlenotes\other-notes\نصائح-وتوصيات-هامة.html</link><guid isPermaLink="false">_ZettleNotes/other Notes/نصائح وتوصيات هامة.md</guid><dc:creator><![CDATA[عبد الرحمن ذاكر الهاشمي]]></dc:creator><pubDate>Tue, 23 Jul 2024 16:14:37 GMT</pubDate></item><item><title><![CDATA[activation function]]></title><description><![CDATA[ 
 <br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/deep learning specialization.md" data-href="_ZettleNotes/programming Notes/Ai Notes/deep learning specialization.md" href="\_zettlenotes\programming-notes\ai-notes\deep-learning-specialization.html" class="internal-link" target="_self" rel="noopener">deep learning specialization</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" data-href="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\activation-function.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/activation function.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:38:41 GMT</pubDate></item><item><title><![CDATA[Adam Algorithm]]></title><description><![CDATA[ 
 <br>not just one <br>definition : ( adaptive moment estimation  )  can adjust learning rate automatically<br>
يعني بيعرف امتي يسرع ال  و امتي يخليها ابطأ و هكذا ...<br>
و بيستخدم  لكل parameter عندك في ال model<br>
how can do that (Abstraction) :<br>
if  or (b) keeps moving in same direction =&gt; increase <br>
if  or (b) keeps oscillating (تتأرجح)  =&gt; reducing <br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/advanced optimization.md" data-href="_ZettleNotes/programming Notes/Ai Notes/advanced optimization.md" href="\_zettlenotes\programming-notes\ai-notes\advanced-optimization.html" class="internal-link" target="_self" rel="noopener">advanced optimization</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\adam-algorithm.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/Adam Algorithm.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:38:28 GMT</pubDate></item><item><title><![CDATA[advanced optimization]]></title><description><![CDATA[ 
 <br><br><br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Adam Algorithm.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Adam Algorithm.md" href="\_zettlenotes\programming-notes\ai-notes\adam-algorithm.html" class="internal-link" target="_self" rel="noopener">Adam Algorithm</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/gradient descent.md" data-href="_ZettleNotes/programming Notes/Ai Notes/gradient descent.md" href="\_zettlenotes\programming-notes\ai-notes\gradient-descent.html" class="internal-link" target="_self" rel="noopener">gradient descent</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" data-href="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\advanced-optimization.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/advanced optimization.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:38:18 GMT</pubDate></item><item><title><![CDATA[Back Propagation]]></title><description><![CDATA[ 
 <br>
<br>الاشتقاق بيعبر عن مقدار التغيير الي بيحصل<br>
computation graph used for calculate cost function and derivative<br>
left to right =&gt; <a data-href="Forward propagation" href="\_zettlenotes\programming-notes\ai-notes\forward-propagation.html" class="internal-link" target="_self" rel="noopener">Forward propagation</a><br>
right to left =&gt; <a data-href="Back Propagation" href="\_zettlenotes\programming-notes\ai-notes\back-propagation.html" class="internal-link" target="_self" rel="noopener">Back Propagation</a>
<br><br>LINKS TO THIS PAGE <br><br><br><br>]]></description><link>_zettlenotes\programming-notes\ai-notes\back-propagation.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/Back Propagation.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:38:13 GMT</pubDate></item><item><title><![CDATA[bias and variance]]></title><description><![CDATA[ 
 <br>Bias and variance are key concepts in machine learning that help to understand and diagnose the performance of a model. They are components of the error in a model, and they play a crucial role in the bias-variance tradeoff, which is central to building models that generalize well to new data. Here’s a detailed explanation of both:<br><br>Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. It represents how well the model's predictions match the actual values.<br>
<br>High Bias: Indicates a model that is too simple, capturing insufficient patterns from the data. This can lead to underfitting, where the model performs poorly both on training data and unseen data. Examples include linear regression on non-linear data.
<br>Low Bias: Indicates a model that captures the patterns in the training data well. However, a very low bias model might be complex and prone to overfitting.
<br><br>Variance refers to the amount by which the model's predictions would change if it were trained on a different dataset. It represents the model’s sensitivity to the specific training data it was trained on.<br>
<br>High Variance: Indicates a model that is too complex, capturing noise along with the underlying patterns in the training data. This can lead to overfitting, where the model performs well on training data but poorly on unseen data.
<br>Low Variance: Indicates a model that is stable with respect to the training data. A model with low variance generally performs consistently across different datasets.
<br><br>The goal in machine learning is to find a model that achieves a good balance between bias and variance, minimizing the total error. The total error in a model can be expressed as:<br>Total&nbsp;Error=Bias2+Variance+Irreducible&nbsp;Error\text{Total Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}Total&nbsp;Error=Bias2+Variance+Irreducible&nbsp;Error<br>
<br>Bias Error: Error due to wrong assumptions in the learning algorithm.
<br>Variance Error: Error due to the model's sensitivity to small fluctuations in the training set.
<br>Irreducible Error: Error that cannot be reduced regardless of the model (e.g., inherent noise in the data).
<br><br>Consider the example of predicting data points on a target:<br>
<br>High Bias, Low Variance: Predictions are consistently wrong in the same way (e.g., missing the target in a systematic way).
<br>Low Bias, High Variance: Predictions are scattered widely around the target, indicating that the model is highly sensitive to training data variations.
<br>High Bias, High Variance: Predictions are both systematically wrong and scattered, leading to poor performance.
<br>Low Bias, Low Variance: Predictions are close to the target and consistent, indicating good performance.
<br><br>
<br>
Underfitting:

<br>High Bias, Low Variance: A linear model on complex data.
<br>The model is too simple and does not capture the underlying trends in the data.
<br>Performance is poor on both training and test data.


<br>
Overfitting:

<br>Low Bias, High Variance: A complex model (e.g., high-degree polynomial) on the same data.
<br>The model captures the noise in the training data.
<br>Performance is good on training data but poor on test data.


<br>
Good Fit:

<br>Moderate Bias, Moderate Variance: A model that captures the underlying trend without fitting noise.
<br>Balanced complexity leads to good performance on both training and test data.


<br><br>
<br>To reduce bias: Use a more complex model or add more features.
<br>To reduce variance: Use techniques like regularization (L1, L2), pruning in decision trees, or ensemble methods like bagging and boosting.
<br><br>Understanding and balancing bias and variance is essential for building effective machine learning models. By carefully managing the complexity of the model and using appropriate techniques, we can build models that generalize well to new, unseen data.<br><br><br><br><br><br>ج : اذا كانت الشبكة منظمة بشكل مناسب <a data-href="Regularization" href="\_zettlenotes\programming-notes\ai-notes\regularization.html" class="internal-link" target="_self" rel="noopener">Regularization</a> فإنها ستعمل بشكل افضل من ال neural network الصغيرة .  <br><br>LINKS TO THIS PAGE <br><br><br><br><br>]]></description><link>_zettlenotes\programming-notes\ai-notes\bias-and-variance.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/bias and variance.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:38:00 GMT</pubDate></item><item><title><![CDATA[binary cross-entropy loss (1)]]></title><description><![CDATA[ 
 <br>It measures the performance of a classification model whose output is a probability value between 0 and 1. The goal is to minimize this cost function during the training process.<br><br><br><br><br>LINKS TO THIS PAGE <br>Dataview: No results to show for list query.]]></description><link>_zettlenotes\programming-notes\ai-notes\binary-cross-entropy-loss-(1).html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/binary cross-entropy loss (1).md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:37:52 GMT</pubDate></item><item><title><![CDATA[clustering]]></title><description><![CDATA[ 
 <br><br>LINKS TO THIS PAGE <br><br>]]></description><link>_zettlenotes\programming-notes\ai-notes\clustering.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/clustering.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:40:07 GMT</pubDate></item><item><title><![CDATA[cross validation]]></title><description><![CDATA[ 
 <br>dev set =  validation set = development set <br>definition : to check accuracy of different models <br>الطريقة كالأتي : <br>we select the best model between some models using training and dev sets after that using testing set to check the actual performance of the model . <br>so ,<br>
training set : used to train the model .<br>
validation set : used to tune hyperparameters and select the best model .<br>
test set : used only once for the final evaluation after the model is fully trained and tuned  . <br><br>Regularization<br>
reduce overfitting by add a penalty<br>
Cross validation<br>
try to add advise and lead the model to correct roadmap (tuning parameters)<br>
is a check point to ensure the model is learning effectively and generalizing well to new data <br><br>LINKS TO THIS PAGE <br><br><br>]]></description><link>_zettlenotes\programming-notes\ai-notes\cross-validation.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/cross validation.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:37:32 GMT</pubDate></item><item><title><![CDATA[First course : Neural Network]]></title><description><![CDATA[ 
 <br>Notes : <br><br>what is deep NN ?<br>
meaning add more hidden layers in the <a data-href="deep learning specialization" href="\_zettlenotes\programming-notes\ai-notes\deep-learning-specialization.html" class="internal-link" target="_self" rel="noopener">deep learning specialization</a><br><img alt="Pasted image 20240723091655.png" src="\lib\media\pasted-image-20240723091655.png" style="width: 500px; max-width: 100%;"><br><a data-href="Forward propagation" href="\_zettlenotes\programming-notes\ai-notes\forward-propagation.html" class="internal-link" target="_self" rel="noopener">Forward propagation</a> equation :<br>
<img alt="Pasted image 20240723092041.png" src="\lib\media\pasted-image-20240723092041.png"><br>Formula : Z<a data-footref="[inline0" href="\#fn-1-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[1]</a> = w<a data-footref="[inline1" href="\#fn-2-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[2]</a> *  A<a data-footref="[inline2" href="\#fn-3-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[3]</a> + b<a data-footref="[inline3" href="\#fn-4-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[4]</a><br>
parameters w<a data-footref="[inline4" href="\#fn-5-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[5]</a> and b<a data-footref="[inline5" href="\#fn-6-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[6]</a> :<br>
w<a data-footref="[inline6" href="\#fn-7-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[7]</a> : (n<a data-footref="[inline7" href="\#fn-8-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[8]</a> , n<a data-footref="[inline8" href="\#fn-9-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[9]</a>)<br>
b<a data-footref="[inline9" href="\#fn-10-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[10]</a> : (n<a data-footref="[inline10" href="\#fn-11-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[11]</a> , 1 )<br>
<a data-href="vectorization" href="\_zettlenotes\programming-notes\ai-notes\vectorization.html" class="internal-link" target="_self" rel="noopener">vectorization</a> :<br>
w<a data-footref="[inline11" href="\#fn-12-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[12]</a> : (n<a data-footref="[inline12" href="\#fn-13-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[13]</a> , n<a data-footref="[inline13" href="\#fn-14-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[14]</a>)<br>
Z = {z<a data-footref="[inline14" href="\#fn-15-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[15]</a><a data-footref="[inline15" href="\#fn-16-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[16]</a> +z<a data-footref="[inline16" href="\#fn-17-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[17]</a><a data-footref="[inline17" href="\#fn-18-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[18]</a> +...+z<a data-footref="[inline18" href="\#fn-19-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[19]</a><a data-footref="[inline19" href="\#fn-20-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[20]</a>  } : (n<a data-footref="[inline20" href="\#fn-21-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[21]</a> , m )<br>
A = {a<a data-footref="[inline21" href="\#fn-22-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[22]</a><a data-footref="[inline22" href="\#fn-23-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[23]</a> +a<a data-footref="[inline23" href="\#fn-24-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[24]</a><a data-footref="[inline24" href="\#fn-25-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[25]</a> +...+a<a data-footref="[inline25" href="\#fn-26-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[26]</a><a data-footref="[inline26" href="\#fn-27-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[27]</a>  } : (n<a data-footref="[inline27" href="\#fn-28-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[28]</a> , m )<br>
b [in python ] = (n<a data-footref="[inline28" href="\#fn-29-3f1389e5676a5cda" class="footnote-link" target="_self" rel="noopener">[29]</a> , m ) <br>building blocks of <a data-href="deep NN" href="\deep NN" class="internal-link" target="_self" rel="noopener">deep NN</a><br>
<img alt="Pasted image 20240723101305.png" src="\lib\media\pasted-image-20240723101305.png"><br>Parameters vs. Hyperparameters :<br>
parameters : w , b<br>
<a data-href="hyperparameters" href="\_zettlenotes\programming-notes\ai-notes\hyperparameters.html" class="internal-link" target="_self" rel="noopener">hyperparameters</a> : learning rate  (alpha) , # iterations , # hidden layers L , # hidden units , choice of <a data-href="activation function" href="\_zettlenotes\programming-notes\ai-notes\activation-function.html" class="internal-link" target="_self" rel="noopener">activation function</a> (RELU , SIGMOID ) , also can be : momentum , mini batch size , <a data-href="Regularization" href="\_zettlenotes\programming-notes\ai-notes\regularization.html" class="internal-link" target="_self" rel="noopener">Regularization</a> <br>Tips &amp; Tricks : <br>
<br>in back propagation the dimension of dw and db must be equal the dimension of w and b in forward propagation . 
<br>when count the number of layers in <a data-href="deep learning specialization" href="\_zettlenotes\programming-notes\ai-notes\deep-learning-specialization.html" class="internal-link" target="_self" rel="noopener">deep learning specialization</a> we count the hidden layers and output layer . 
<br><br><br>Notes : <br><br><img alt="Pasted image 20240727085600.png#center" src="\lib\media\pasted-image-20240727085600.png" style="width: 500px; max-width: 100%;"><br>
we aim to improve this cycle<br>
لذلك يمكننا تطوير و رفع كفائة هذه الدائرة و من ضمن الاشياء هذه هو تقسيم الداتا سيت الي ثلاث اقسام اساسية : training set / validation set / testing set<br>
و كذلك سيسمح لك بقياس اكثر فاعلية لتحيز و تباين خوارزميتك <br>
<br>
في data sets الصغيرة نسبة توزيع الثلاث اقسام يكون جيد تطبيق التوزيع التقليدي :<br>
60 , 20 , 20 % 

<br>
اما في حالة كانت الداتا كبيره مثلا مليون او اكثر فيفضل تقسيم السيتس الي :<br>
90 , 5 , 5 %<br>
فيفضل توزيع ال dev and testing set بنسبة اقل من 20 او حتي اقل من 10 في المية من الداتا الكبيرة 

<br><br>
<br>
make sure dev and test come from same distribution 

<br>
not having a test set might be okay . (only dev set . ) . 

<br>Tips and Tricks : <br>
<br>the goal of testing set is avoid unbiased estimate of the performance of the model . 
<br>validation / dev set : to test multiple models and choose what is the best model . 
<br><br>العوامل الاساسية في تحديد ما اذا حدث bias or variance من خلال  <br><br><br><br>solve bias problem : <br><br>solve variance problem  : <br><br><br>how to apply <a data-href="Regularization" href="\_zettlenotes\programming-notes\ai-notes\regularization.html" class="internal-link" target="_self" rel="noopener">Regularization</a> in <a data-href="deep NN" href="\deep NN" class="internal-link" target="_self" rel="noopener">deep NN</a> , and what is the types of regularization <br><br>setting up your optimization problem :<br>
<a data-href="Normalizing" href="\_zettlenotes\programming-notes\ai-notes\normalizing.html" class="internal-link" target="_self" rel="noopener">Normalizing</a><br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/activation function.md" data-href="_ZettleNotes/programming Notes/Ai Notes/activation function.md" href="\_zettlenotes\programming-notes\ai-notes\activation-function.html" class="internal-link" target="_self" rel="noopener">activation function</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/bias and variance.md" data-href="_ZettleNotes/programming Notes/Ai Notes/bias and variance.md" href="\_zettlenotes\programming-notes\ai-notes\bias-and-variance.html" class="internal-link" target="_self" rel="noopener">bias and variance</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Forward propagation.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Forward propagation.md" href="\_zettlenotes\programming-notes\ai-notes\forward-propagation.html" class="internal-link" target="_self" rel="noopener">Forward propagation</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/hyperparameters.md" data-href="_ZettleNotes/programming Notes/Ai Notes/hyperparameters.md" href="\_zettlenotes\programming-notes\ai-notes\hyperparameters.html" class="internal-link" target="_self" rel="noopener">hyperparameters</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/other Notes/Index - data science.md" data-href="_ZettleNotes/other Notes/Index - data science.md" href="\_zettlenotes\other-notes\index-data-science.html" class="internal-link" target="_self" rel="noopener">Index - data science</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Normalizing.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Normalizing.md" href="\_zettlenotes\programming-notes\ai-notes\normalizing.html" class="internal-link" target="_self" rel="noopener">Normalizing</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Regularization.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Regularization.md" href="\_zettlenotes\programming-notes\ai-notes\regularization.html" class="internal-link" target="_self" rel="noopener">Regularization</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/vectorization.md" data-href="_ZettleNotes/programming Notes/Ai Notes/vectorization.md" href="\_zettlenotes\programming-notes\ai-notes\vectorization.html" class="internal-link" target="_self" rel="noopener">vectorization</a><br>
<br>
<br>L<a href="\#fnref-1-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>L<a href="\#fnref-2-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>L<a href="\#fnref-3-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>L<a href="\#fnref-4-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>L<a href="\#fnref-5-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>L<a href="\#fnref-6-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>L<a href="\#fnref-7-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>L<a href="\#fnref-8-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>L-1<a href="\#fnref-9-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>L<a href="\#fnref-10-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>L<a href="\#fnref-11-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>L<a href="\#fnref-12-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>L<a href="\#fnref-13-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>L-1<a href="\#fnref-14-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>1<a href="\#fnref-15-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>1<a href="\#fnref-16-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>1<a href="\#fnref-17-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>2<a href="\#fnref-18-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>1<a href="\#fnref-19-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>m<a href="\#fnref-20-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>L<a href="\#fnref-21-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>1<a href="\#fnref-22-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>1<a href="\#fnref-23-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>1<a href="\#fnref-24-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>2<a href="\#fnref-25-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>1<a href="\#fnref-26-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>m<a href="\#fnref-27-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>L-1<a href="\#fnref-28-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
<br>L<a href="\#fnref-29-3f1389e5676a5cda" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
]]></description><link>_zettlenotes\programming-notes\ai-notes\deep-learning-specialization.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/deep learning specialization.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:37:08 GMT</pubDate><enclosure url="lib\media\pasted-image-20240723091655.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240723091655.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[feature selection]]></title><description><![CDATA[ 
 <br><br>LINKS TO THIS PAGE <br><br><br>]]></description><link>_zettlenotes\programming-notes\ai-notes\feature-selection.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/feature selection.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:36:57 GMT</pubDate></item><item><title><![CDATA[Forward propagation]]></title><description><![CDATA[ 
 <br>Forward propagation is a key process in neural networks, used to compute the output of the network given an input. It involves passing the input data through the network's layers to obtain the final prediction. Here's a step-by-step overview of how forward propagation works:<br><br>The process begins with the input layer, where the input data (features) is fed into the network.<br><br>
<br>Each hidden layer consists of multiple neurons, and each neuron receives input from the previous layer.
<br>For each neuron in a hidden layer, the input data is multiplied by the neuron's weights and summed. This sum is often referred to as the weighted sum.
<br>A bias term is added to the weighted sum to shift the activation function. This helps the network model complex patterns more effectively.
<br>The result is passed through an activation function (e.g., ReLU, sigmoid, tanh) to introduce non-linearity. The activation function's output becomes the input for the next layer.
<br><br>The final hidden layer's outputs are passed to the output layer.<br>
<br>In the output layer, a similar process occurs where the weighted sum and activation function determine the network's final output.
<br>The output layer's activation function depends on the specific task. For example, a softmax activation is often used for multi-class classification tasks to produce probabilities, while a linear activation might be used for regression tasks.
<br><br>For a simple network with one hidden layer:<br>Input to Hidden Layer:<br><br>Hidden Layer to Output Layer:<br> <br>Where:<br>
<br>xxx is the input vector.
<br>W(1)W^{(1)}W(1) and W(2)W^{(2)}W(2) are the weight matrices for the hidden and output layers, respectively.
<br>b(1)b^{(1)}b(1) and b(2)b^{(2)}b(2) are the bias vectors for the hidden and output layers, respectively.
<br>zzz represents the weighted sum before applying the activation function.
<br>aaa represents the output after applying the activation function (activation output).
<br>σ\sigmaσ is the activation function for the hidden layer.
<br><br>Consider a network with a single input neuron, one hidden layer with two neurons, and one output neuron:<br>Input Layer:<br> <br>Hidden Layer:<br> <br>Output Layer:<br><br>Forward propagation continues through all layers until the final output is produced. This output is then used for making predictions, calculating loss, or further processing depending on the specific application.<br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Back Propagation.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Back Propagation.md" href="\_zettlenotes\programming-notes\ai-notes\back-propagation.html" class="internal-link" target="_self" rel="noopener">Back Propagation</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/deep learning specialization.md" data-href="_ZettleNotes/programming Notes/Ai Notes/deep learning specialization.md" href="\_zettlenotes\programming-notes\ai-notes\deep-learning-specialization.html" class="internal-link" target="_self" rel="noopener">deep learning specialization</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" data-href="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\forward-propagation.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/Forward propagation.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:36:37 GMT</pubDate></item><item><title><![CDATA[gradient descent]]></title><description><![CDATA[ 
 <br>definition : mechanism used in training data to minimize loss function ( find local minimum )<br>
لانك لما تبدأ من مكان مختلف في كل مره هتلاقي نفسك و صلت لقاع (وادي ) مختلف عن الي قبلهم . <br><br> =&gt; [0,1]  0,01<br>
=&gt; بيحدد الاتجاه الي همشي فيه <br><br>correct  simulate new update <br>temp - w = w - a d/dw J(w,b) 
temp - b = b - a d/dw J(w,b) 
w = temp-w 
b = temp - b 
Copy<br>in correct simulate new update <br>temp - w = w - a d/dw J(w,b) 
w = temp-w 
temp - b = b - a d/dw J(w,b) 
b = temp - b 
Copy<br>الغلط في المثال التاني اني مادام شغال في simulate ازاي احط القيمة الجديدة بتاعة ال w في ال b الي المفروض تكون الحدث في نفس الوقت الي w  بتحدث فيه . <br><img alt="Pasted image 20240730234540.png#center" src="\lib\media\pasted-image-20240730234540.png" style="width: 500px; max-width: 100%;"><br>in <a data-href="gradient descent" href="\_zettlenotes\programming-notes\ai-notes\gradient-descent.html" class="internal-link" target="_self" rel="noopener">gradient descent</a> can reach minimum without decreasing learning rate<br>
عشان كلما اقترب من ال local minimum الاشتقاق بيقل لحد ما يوصل للصفر<br>
<img alt="Pasted image 20240730235441.png#center" src="\lib\media\pasted-image-20240730235441.png" style="width: 500px; max-width: 100%;"><br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/advanced optimization.md" data-href="_ZettleNotes/programming Notes/Ai Notes/advanced optimization.md" href="\_zettlenotes\programming-notes\ai-notes\advanced-optimization.html" class="internal-link" target="_self" rel="noopener">advanced optimization</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/linear Regression.md" data-href="_ZettleNotes/programming Notes/Ai Notes/linear Regression.md" href="\_zettlenotes\programming-notes\ai-notes\linear-regression.html" class="internal-link" target="_self" rel="noopener">linear Regression</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Vanishing and Exploding gradients.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Vanishing and Exploding gradients.md" href="\_zettlenotes\programming-notes\ai-notes\vanishing-and-exploding-gradients.html" class="internal-link" target="_self" rel="noopener">Vanishing and Exploding gradients</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\gradient-descent.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/gradient descent.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:36:27 GMT</pubDate><enclosure url="lib\media\pasted-image-20240730234540.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240730234540.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[hyperparameters]]></title><description><![CDATA[ 
 <br><br>hyperparameters<br>
optimization of hyper parameters like (learning rate , batch size  , N of layers , N of neurons ,  <a data-href="Regularization" href="\_zettlenotes\programming-notes\ai-notes\regularization.html" class="internal-link" target="_self" rel="noopener">Regularization</a> parameters )<br>
focuses on finding the best hyperparameters that control the training process and model to achieve the best performance <br>gradient descent<br>
optimization of model parameters<br>
focuses on adjusting the model parameters to minimize the loss function during training . <br><br>LINKS TO THIS PAGE <br><br><br>]]></description><link>_zettlenotes\programming-notes\ai-notes\hyperparameters.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/hyperparameters.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:36:14 GMT</pubDate></item><item><title><![CDATA[learning curves]]></title><description><![CDATA[ 
 <br>base line : what is the level of error you can reasonably hope to get to ? <br><br><img alt="Pasted image 20240731040247.png" src="\lib\media\pasted-image-20240731040247.png"><br>كلما زاد عدد الامثلة كلما زاد نسبة الخطأ الخاصة بال train لانه يصعب عليه عمل fit لكل الامثلة .<br>
و لكن في حالة ان ال model به high bias فإن زيادة حجم ال training set لن يؤثر علي ال model .<br>
if a learning algorithm suffers from high variance , getting more training data is likely to help . <br><br>LINKS TO THIS PAGE <br><br>]]></description><link>_zettlenotes\programming-notes\ai-notes\learning-curves.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/learning curves.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:36:07 GMT</pubDate><enclosure url="lib\media\pasted-image-20240731040247.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240731040247.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Linear function]]></title><description><![CDATA[ 
 <br><img alt="Pasted image 20240731023417.png" src="\lib\media\pasted-image-20240731023417.png"><br>no activation function <br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" data-href="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\linear-function.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/Linear function.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:35:50 GMT</pubDate><enclosure url="lib\media\pasted-image-20240731023417.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240731023417.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[linear Regression]]></title><description><![CDATA[ 
 <br><br><br><img alt="Pasted image 20240730231105.png#center" src="\lib\media\pasted-image-20240730231105.png" style="width: 500px; max-width: 100%;"><br>w , b : coefficients parameters weights<br><br><br><a data-href="Mean Squared Error (MSE)" href="\_zettlenotes\programming-notes\ai-notes\mean-squared-error-(mse).html" class="internal-link" target="_self" rel="noopener">Mean Squared Error (MSE)</a><br><br><br><a data-href="gradient descent" href="\_zettlenotes\programming-notes\ai-notes\gradient-descent.html" class="internal-link" target="_self" rel="noopener">gradient descent</a><br><br><br> =&gt; <a data-href="vectorization" href="\_zettlenotes\programming-notes\ai-notes\vectorization.html" class="internal-link" target="_self" rel="noopener">vectorization</a> format is efficiency for coding <br>multiple linear regression  (No multivariant regression  )<br>alternative to <a data-href="gradient descent" href="\_zettlenotes\programming-notes\ai-notes\gradient-descent.html" class="internal-link" target="_self" rel="noopener">gradient descent</a> in <a data-href="linear Regression" href="\_zettlenotes\programming-notes\ai-notes\linear-regression.html" class="internal-link" target="_self" rel="noopener">linear Regression</a> is <a data-href="Normal Equation" href="\_zettlenotes\programming-notes\ai-notes\normal-equation.html" class="internal-link" target="_self" rel="noopener">Normal Equation</a> <br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/gradient descent.md" data-href="_ZettleNotes/programming Notes/Ai Notes/gradient descent.md" href="\_zettlenotes\programming-notes\ai-notes\gradient-descent.html" class="internal-link" target="_self" rel="noopener">gradient descent</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Logistic Regression (Classification ).md" data-href="_ZettleNotes/programming Notes/Ai Notes/Logistic Regression (Classification ).md" href="\_zettlenotes\programming-notes\ai-notes\logistic-regression-(classification-).html" class="internal-link" target="_self" rel="noopener">Logistic Regression (Classification )</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Mean Squared Error (MSE).md" data-href="_ZettleNotes/programming Notes/Ai Notes/Mean Squared Error (MSE).md" href="\_zettlenotes\programming-notes\ai-notes\mean-squared-error-(mse).html" class="internal-link" target="_self" rel="noopener">Mean Squared Error (MSE)</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" data-href="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Normal Equation.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Normal Equation.md" href="\_zettlenotes\programming-notes\ai-notes\normal-equation.html" class="internal-link" target="_self" rel="noopener">Normal Equation</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Supervised learning.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Supervised learning.md" href="\_zettlenotes\programming-notes\ai-notes\supervised-learning.html" class="internal-link" target="_self" rel="noopener">Supervised learning</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/vectorization.md" data-href="_ZettleNotes/programming Notes/Ai Notes/vectorization.md" href="\_zettlenotes\programming-notes\ai-notes\vectorization.html" class="internal-link" target="_self" rel="noopener">vectorization</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\linear-regression.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/linear Regression.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:35:36 GMT</pubDate><enclosure url="lib\media\pasted-image-20240730231105.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240730231105.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Logistic Regression (Classification )]]></title><description><![CDATA[ 
 <br>definition : <a data-href="Logistic Regression (Classification )" href="\_zettlenotes\programming-notes\ai-notes\logistic-regression-(classification-).html" class="internal-link" target="_self" rel="noopener">Logistic Regression (Classification )</a> is a statistical method used for binary classification problems, where the goal is to predict the probability that a given input belongs to one of two possible classes. Unlike<a data-href="linear Regression" href="\_zettlenotes\programming-notes\ai-notes\linear-regression.html" class="internal-link" target="_self" rel="noopener">linear Regression</a>, which predicts a continuous output, logistic regression predicts a probability that is then mapped to a discrete class label.<br>use logistic function (<a data-href="sigmoid function" href="\_zettlenotes\programming-notes\ai-notes\sigmoid-function.html" class="internal-link" target="_self" rel="noopener">sigmoid function</a> )<br>think in logistic regression the probability that class is 1 <br><br><br>another format : <br><br>the probability that y is 1 given input , parameters , b <br><br><br>
<br><br><a data-href="Binary Cross-Entropy Loss" href="\_zettlenotes\binary-cross-entropy-loss.html" class="internal-link" target="_self" rel="noopener">Binary Cross-Entropy Loss</a><br><br><br><br><a data-href="underverfitting &amp; overfitting" href="\_zettlenotes\programming-notes\ai-notes\underverfitting-&amp;-overfitting.html" class="internal-link" target="_self" rel="noopener">underverfitting &amp; overfitting</a><br>
<a data-href="Softmax regression" href="\_zettlenotes\programming-notes\ai-notes\softmax-regression.html" class="internal-link" target="_self" rel="noopener">Softmax regression</a><br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/Binary Cross-Entropy Loss.md" data-href="_ZettleNotes/Binary Cross-Entropy Loss.md" href="\_zettlenotes\binary-cross-entropy-loss.html" class="internal-link" target="_self" rel="noopener">Binary Cross-Entropy Loss</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/linear Regression.md" data-href="_ZettleNotes/programming Notes/Ai Notes/linear Regression.md" href="\_zettlenotes\programming-notes\ai-notes\linear-regression.html" class="internal-link" target="_self" rel="noopener">linear Regression</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" data-href="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/sigmoid function.md" data-href="_ZettleNotes/programming Notes/Ai Notes/sigmoid function.md" href="\_zettlenotes\programming-notes\ai-notes\sigmoid-function.html" class="internal-link" target="_self" rel="noopener">sigmoid function</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Softmax regression.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Softmax regression.md" href="\_zettlenotes\programming-notes\ai-notes\softmax-regression.html" class="internal-link" target="_self" rel="noopener">Softmax regression</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Supervised learning.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Supervised learning.md" href="\_zettlenotes\programming-notes\ai-notes\supervised-learning.html" class="internal-link" target="_self" rel="noopener">Supervised learning</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/underverfitting &amp; overfitting.md" data-href="_ZettleNotes/programming Notes/Ai Notes/underverfitting &amp; overfitting.md" href="\_zettlenotes\programming-notes\ai-notes\underverfitting-&amp;-overfitting.html" class="internal-link" target="_self" rel="noopener">underverfitting &amp; overfitting</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\logistic-regression-(classification-).html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/Logistic Regression (Classification ).md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:35:31 GMT</pubDate></item><item><title><![CDATA[loop of ML development]]></title><description><![CDATA[ 
 <br><img alt="Pasted image 20240731041721.png" src="\lib\media\pasted-image-20240731041721.png"><br><br> = 500<br>
ممكن مثلا ال model يصنف 100 من اصل 500 غلط ، فانت ك ml developer هتصنفهم انت بنفسك و تحاول تعدل في ال Architectures  بتاعة ال model<br>
Note that : Adding more data cause to increase time cost .<br>
فبدل ما اضيف داتا for every thing لا ، احنا هنضيف داتا data types where error analysis  . <br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" data-href="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\loop-of-ml-development.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/loop of ML development.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:35:04 GMT</pubDate><enclosure url="lib\media\pasted-image-20240731041721.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240731041721.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[machine learning specialization]]></title><description><![CDATA[<a class="tag" href="?query=tag:Arthur_Samuel" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Arthur_Samuel</a> 
 <br>machine learning : "Field of study that gives computers the ability to learn without being explicitly programmed " - <a href=".?query=tag:Arthur_Samuel" class="tag" target="_blank" rel="noopener">#Arthur_Samuel</a> 1959 <br><br><br><a data-href="neural network NN" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a><br><br>how to evaluate the performance of learning algorithm <br>
<br>split training set to 70 % training set and 30 % testing set 
<br>note that in training sets and testing sets we not add the <a data-href="Regularization" href="\_zettlenotes\programming-notes\ai-notes\regularization.html" class="internal-link" target="_self" rel="noopener">Regularization</a> term , because we want show the performance of the models themselves 
<br><br>فيه مشكلة انك لو اخترت علي اساس ال training set فهيكون النسبة زيادة عن اللزوم و كذلك لو اخترت علي اساس ال testing set  بس هتلاقي النسبة زيادة عن اللزوم برده ، فالحل في ال <a data-href="cross validation" href="\_zettlenotes\programming-notes\ai-notes\cross-validation.html" class="internal-link" target="_self" rel="noopener">cross validation</a><br>
split data into three parts <br><br><a data-href="learning curves" href="\_zettlenotes\programming-notes\ai-notes\learning-curves.html" class="internal-link" target="_self" rel="noopener">learning curves</a><br><br>LINKS TO THIS PAGE <br><br><br><br><br><br><br><br>]]></description><link>_zettlenotes\programming-notes\ai-notes\machine-learning-specialization.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/machine learning specialization.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:34:32 GMT</pubDate></item><item><title><![CDATA[Mean Squared Error (MSE)]]></title><description><![CDATA[ 
 <br>loss  function is <a data-href="convex" href="\convex" class="internal-link" target="_self" rel="noopener">convex</a> cost function<br>
cost function is summation of loss function <br><br>
<br>the error or different between y &amp;  is the vertical line between y and  
<br><br>It is especially useful when dealing with data that contains both positive and negative values, as it gives a single value representing the overall magnitude.<br><br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/linear Regression.md" data-href="_ZettleNotes/programming Notes/Ai Notes/linear Regression.md" href="\_zettlenotes\programming-notes\ai-notes\linear-regression.html" class="internal-link" target="_self" rel="noopener">linear Regression</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" data-href="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\mean-squared-error-(mse).html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/Mean Squared Error (MSE).md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:34:20 GMT</pubDate></item><item><title><![CDATA[multiclass classification]]></title><description><![CDATA[ 
 <br>classification can have more just two classification , not just o or 1 like handwritten digit .<br>
so , we can solve the problem using <a data-href="Softmax regression" href="\_zettlenotes\programming-notes\ai-notes\softmax-regression.html" class="internal-link" target="_self" rel="noopener">Softmax regression</a><br><br>LINKS TO THIS PAGE <br><br><br><br>]]></description><link>_zettlenotes\programming-notes\ai-notes\multiclass-classification.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/multiclass classification.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:34:06 GMT</pubDate></item><item><title><![CDATA[multilabel label classification]]></title><description><![CDATA[ 
 <br>يعني ازاي تعمل اكتر من تصنيف <br><img alt="Pasted image 20240731030726.png" src="\lib\media\pasted-image-20240731030726.png" style="width: 400px; max-width: 100%;"><br><a data-href="multiclass classification" href="\_zettlenotes\programming-notes\ai-notes\multiclass-classification.html" class="internal-link" target="_self" rel="noopener">multiclass classification</a> vs <a data-href="multilabel label classification" href="\_zettlenotes\programming-notes\ai-notes\multilabel-label-classification.html" class="internal-link" target="_self" rel="noopener">multilabel label classification</a><br>
multi class<br>
اختيار 1 فقط من ال categorize الي عندك و غالبا بنستخدم <a data-href="Softmax regression" href="\_zettlenotes\programming-notes\ai-notes\softmax-regression.html" class="internal-link" target="_self" rel="noopener">Softmax regression</a><br>
multi label<br>
اختيار اكتر من categorize مع بعض و غالباً بنستخدم ال <a data-href="sigmoid function" href="\_zettlenotes\programming-notes\ai-notes\sigmoid-function.html" class="internal-link" target="_self" rel="noopener">sigmoid function</a><br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/multiclass classification.md" data-href="_ZettleNotes/programming Notes/Ai Notes/multiclass classification.md" href="\_zettlenotes\programming-notes\ai-notes\multiclass-classification.html" class="internal-link" target="_self" rel="noopener">multiclass classification</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" data-href="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/sigmoid function.md" data-href="_ZettleNotes/programming Notes/Ai Notes/sigmoid function.md" href="\_zettlenotes\programming-notes\ai-notes\sigmoid-function.html" class="internal-link" target="_self" rel="noopener">sigmoid function</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Softmax regression.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Softmax regression.md" href="\_zettlenotes\programming-notes\ai-notes\softmax-regression.html" class="internal-link" target="_self" rel="noopener">Softmax regression</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\multilabel-label-classification.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/multilabel label classification.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:33:36 GMT</pubDate><enclosure url="lib\media\pasted-image-20240731030726.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240731030726.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[neural network NN]]></title><description><![CDATA[ 
 <br><img alt="Pasted image 20240731015629.png" src="\lib\media\pasted-image-20240731015629.png"><br><br><br>Neural Network component :<br>
<br>input layers 
<br>hidden layers 
<br>output layers 
<br><br><img alt="Pasted image 20240731020023.png" src="\lib\media\pasted-image-20240731020023.png"><br>
<a data-href="activation function" href="\_zettlenotes\programming-notes\ai-notes\activation-function.html" class="internal-link" target="_self" rel="noopener">activation function</a><br><br><br><img alt="Pasted image 20240731020843.png" src="\lib\media\pasted-image-20240731020843.png"><br><br>عند حساب عدد ال layers مش بنحسب ال inputs layers . <br><br>we use <a data-href="sigmoid function" href="\_zettlenotes\programming-notes\ai-notes\sigmoid-function.html" class="internal-link" target="_self" rel="noopener">sigmoid function</a> as an <a data-href="activation function" href="\_zettlenotes\programming-notes\ai-notes\activation-function.html" class="internal-link" target="_self" rel="noopener">activation function</a><br><a data-href="Forward propagation" href="\_zettlenotes\programming-notes\ai-notes\forward-propagation.html" class="internal-link" target="_self" rel="noopener">Forward propagation</a><br><a data-href="vectorization" href="\_zettlenotes\programming-notes\ai-notes\vectorization.html" class="internal-link" target="_self" rel="noopener">vectorization</a><br>
<br>خلي بالك في ان ال w كانت vector في ال <a data-href="linear Regression" href="\_zettlenotes\programming-notes\ai-notes\linear-regression.html" class="internal-link" target="_self" rel="noopener">linear Regression</a> and <a data-href="Logistic Re`gression (Classification )" href="\Logistic Re`gression (Classification )" class="internal-link" target="_self" rel="noopener">Logistic Re`gression (Classification )</a> و لكن في ال <a data-href="neural network NN" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a> بقت matrix . 
<br><br>
<br>create a Neural Network " how to compute output give input x and parameters "
model = sequential([layer 1  , layer 2 ])
layer : dense(units = , activation = )
Copy

<br>specify the loss function 
model.compile(loss = BinaryCrossEntropy()) 
Copy

<br>fit the first step using the second step to set data (x, y ) 
model.fit(x,y,epoches = 100 ) , #epoches =&gt; number of steps in gradient descent  
Copy

<br>note that loss function for 1 example , but cost function take average of loss function <br>if is the problem can handle true or not use logistic loss known ass <a data-href="Binary Cross-Entropy Loss" href="\_zettlenotes\binary-cross-entropy-loss.html" class="internal-link" target="_self" rel="noopener">Binary Cross-Entropy Loss</a><br>if is the problem can handle predict the value <a data-href="Mean Squared Error (MSE)" href="\_zettlenotes\programming-notes\ai-notes\mean-squared-error-(mse).html" class="internal-link" target="_self" rel="noopener">Mean Squared Error (MSE)</a><br><br>we compute derivatives for gradient descent using <a data-href="Back Propagation" href="\_zettlenotes\programming-notes\ai-notes\back-propagation.html" class="internal-link" target="_self" rel="noopener">Back Propagation</a> <br>model.fit(x,y,epoches  = 100 ) 
#doing backpropagation also 
Copy<br><br><br>choosing g(z) for hidden layers ?<br>
<br><a data-href="Relu Functon" href="\_zettlenotes\programming-notes\ai-notes\relu-functon.html" class="internal-link" target="_self" rel="noopener">Relu Functon</a> most common choice why not <a data-href="sigmoid function" href="\_zettlenotes\programming-notes\ai-notes\sigmoid-function.html" class="internal-link" target="_self" rel="noopener">sigmoid function</a> ?<br>
1. relu faster than sigmoid<br>
2. faster learning because relu have one flat region , but sigmoid have two flat region  , the flat region make the gradient descent be slower<br>
choosing g(z) for outputs layer ?<br>
<a data-href="sigmoid function" href="\_zettlenotes\programming-notes\ai-notes\sigmoid-function.html" class="internal-link" target="_self" rel="noopener">sigmoid function</a> =&gt; <a data-href="Logistic Regression (Classification )" href="\_zettlenotes\programming-notes\ai-notes\logistic-regression-(classification-).html" class="internal-link" target="_self" rel="noopener">Logistic Regression (Classification )</a><br>
<a data-href="Linear function" href="\_zettlenotes\programming-notes\ai-notes\linear-function.html" class="internal-link" target="_self" rel="noopener">Linear function</a> =&gt; <a data-href="linear Regression" href="\_zettlenotes\programming-notes\ai-notes\linear-regression.html" class="internal-link" target="_self" rel="noopener">linear Regression</a><br>
<a data-href="Relu Functon" href="\_zettlenotes\programming-notes\ai-notes\relu-functon.html" class="internal-link" target="_self" rel="noopener">Relu Functon</a> =&gt; <a data-href="linear Regression" href="\_zettlenotes\programming-notes\ai-notes\linear-regression.html" class="internal-link" target="_self" rel="noopener">linear Regression</a>
<br>why do we need activation functions ?<br>
because Neural Network not be able to fit any things more complex rather than <a data-href="Linear function" href="\_zettlenotes\programming-notes\ai-notes\linear-function.html" class="internal-link" target="_self" rel="noopener">Linear function</a>( no activation function )<br><br><br><br><br><br>each neuron output is a function of all the activation outputs of the previous layer<br>
<br><br>each neuron only looks at part of the previous layer's outputs .<br>
that make it : <br>
<br>faster computation 
<br>need less training data (less prone to overfitting) 
<br><a data-href="Back Propagation" href="\_zettlenotes\programming-notes\ai-notes\back-propagation.html" class="internal-link" target="_self" rel="noopener">Back Propagation</a><br>
<a data-href="bias and variance" href="\_zettlenotes\programming-notes\ai-notes\bias-and-variance.html" class="internal-link" target="_self" rel="noopener">bias and variance</a><br>
<a data-href="loop of ML development" href="\_zettlenotes\programming-notes\ai-notes\loop-of-ml-development.html" class="internal-link" target="_self" rel="noopener">loop of ML development</a><br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/activation function.md" data-href="_ZettleNotes/programming Notes/Ai Notes/activation function.md" href="\_zettlenotes\programming-notes\ai-notes\activation-function.html" class="internal-link" target="_self" rel="noopener">activation function</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/advanced optimization.md" data-href="_ZettleNotes/programming Notes/Ai Notes/advanced optimization.md" href="\_zettlenotes\programming-notes\ai-notes\advanced-optimization.html" class="internal-link" target="_self" rel="noopener">advanced optimization</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Back Propagation.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Back Propagation.md" href="\_zettlenotes\programming-notes\ai-notes\back-propagation.html" class="internal-link" target="_self" rel="noopener">Back Propagation</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/bias and variance.md" data-href="_ZettleNotes/programming Notes/Ai Notes/bias and variance.md" href="\_zettlenotes\programming-notes\ai-notes\bias-and-variance.html" class="internal-link" target="_self" rel="noopener">bias and variance</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/Binary Cross-Entropy Loss.md" data-href="_ZettleNotes/Binary Cross-Entropy Loss.md" href="\_zettlenotes\binary-cross-entropy-loss.html" class="internal-link" target="_self" rel="noopener">Binary Cross-Entropy Loss</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Forward propagation.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Forward propagation.md" href="\_zettlenotes\programming-notes\ai-notes\forward-propagation.html" class="internal-link" target="_self" rel="noopener">Forward propagation</a><br><a data-tooltip-position="top" aria-label="index.md" data-href="index.md" href="\index.html" class="internal-link" target="_self" rel="noopener">index</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Linear function.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Linear function.md" href="\_zettlenotes\programming-notes\ai-notes\linear-function.html" class="internal-link" target="_self" rel="noopener">Linear function</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/linear Regression.md" data-href="_ZettleNotes/programming Notes/Ai Notes/linear Regression.md" href="\_zettlenotes\programming-notes\ai-notes\linear-regression.html" class="internal-link" target="_self" rel="noopener">linear Regression</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Logistic Regression (Classification ).md" data-href="_ZettleNotes/programming Notes/Ai Notes/Logistic Regression (Classification ).md" href="\_zettlenotes\programming-notes\ai-notes\logistic-regression-(classification-).html" class="internal-link" target="_self" rel="noopener">Logistic Regression (Classification )</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/loop of ML development.md" data-href="_ZettleNotes/programming Notes/Ai Notes/loop of ML development.md" href="\_zettlenotes\programming-notes\ai-notes\loop-of-ml-development.html" class="internal-link" target="_self" rel="noopener">loop of ML development</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/machine learning specialization.md" data-href="_ZettleNotes/programming Notes/Ai Notes/machine learning specialization.md" href="\_zettlenotes\programming-notes\ai-notes\machine-learning-specialization.html" class="internal-link" target="_self" rel="noopener">machine learning specialization</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Mean Squared Error (MSE).md" data-href="_ZettleNotes/programming Notes/Ai Notes/Mean Squared Error (MSE).md" href="\_zettlenotes\programming-notes\ai-notes\mean-squared-error-(mse).html" class="internal-link" target="_self" rel="noopener">Mean Squared Error (MSE)</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/multiclass classification.md" data-href="_ZettleNotes/programming Notes/Ai Notes/multiclass classification.md" href="\_zettlenotes\programming-notes\ai-notes\multiclass-classification.html" class="internal-link" target="_self" rel="noopener">multiclass classification</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/multilabel label classification.md" data-href="_ZettleNotes/programming Notes/Ai Notes/multilabel label classification.md" href="\_zettlenotes\programming-notes\ai-notes\multilabel-label-classification.html" class="internal-link" target="_self" rel="noopener">multilabel label classification</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Regularization.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Regularization.md" href="\_zettlenotes\programming-notes\ai-notes\regularization.html" class="internal-link" target="_self" rel="noopener">Regularization</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Relu Functon.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Relu Functon.md" href="\_zettlenotes\programming-notes\ai-notes\relu-functon.html" class="internal-link" target="_self" rel="noopener">Relu Functon</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/sigmoid function.md" data-href="_ZettleNotes/programming Notes/Ai Notes/sigmoid function.md" href="\_zettlenotes\programming-notes\ai-notes\sigmoid-function.html" class="internal-link" target="_self" rel="noopener">sigmoid function</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Softmax regression.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Softmax regression.md" href="\_zettlenotes\programming-notes\ai-notes\softmax-regression.html" class="internal-link" target="_self" rel="noopener">Softmax regression</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Vanishing and Exploding gradients.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Vanishing and Exploding gradients.md" href="\_zettlenotes\programming-notes\ai-notes\vanishing-and-exploding-gradients.html" class="internal-link" target="_self" rel="noopener">Vanishing and Exploding gradients</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/vectorization.md" data-href="_ZettleNotes/programming Notes/Ai Notes/vectorization.md" href="\_zettlenotes\programming-notes\ai-notes\vectorization.html" class="internal-link" target="_self" rel="noopener">vectorization</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\neural-network-nn.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/neural network NN.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Fri, 02 Aug 2024 13:34:10 GMT</pubDate><enclosure url="lib\media\pasted-image-20240731015629.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240731015629.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Normal Equation]]></title><description><![CDATA[ 
 <br>The Normal Equation is a method used in linear regression to find the parameters (weights) that minimize the cost function. It provides a direct solution to the least squares problem without the need for iterative optimization algorithms like gradient descent. The Normal Equation is derived from setting the gradient of the cost function to zero and solving for the parameters.<br>that is for any <a data-href="linear Regression" href="\_zettlenotes\programming-notes\ai-notes\linear-regression.html" class="internal-link" target="_self" rel="noopener">linear Regression</a><br><br><br><br>slow when Number of features is large &gt; 10,000 <br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/linear Regression.md" data-href="_ZettleNotes/programming Notes/Ai Notes/linear Regression.md" href="\_zettlenotes\programming-notes\ai-notes\linear-regression.html" class="internal-link" target="_self" rel="noopener">linear Regression</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\normal-equation.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/Normal Equation.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:33:13 GMT</pubDate></item><item><title><![CDATA[Normalizing]]></title><description><![CDATA[ 
 <br>Normalization in machine learning refers to the process of scaling input features to a common range or distribution, which can help improve the performance and stability of models. It involves transforming the data so that it meets certain statistical properties or falls within a specific range. Here are the key concepts:<br><br><br><br><br><br>uses same  and  to normalize test set <br><img alt="Pasted image 20240729042055.png" src="\lib\media\pasted-image-20240729042055.png"><br><br><br><br>
<br>Purpose: Scales data to a fixed range, typically [0, 1].
<br>Formula: 


<br><br>x = \frac{x - \mu }{\sigma}<br><br>x = \frac{x - median(x)}{IQR(x)}<br>]]></description><link>_zettlenotes\programming-notes\ai-notes\normalizing.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/Normalizing.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:32:44 GMT</pubDate><enclosure url="lib\media\pasted-image-20240729042055.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240729042055.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[orthogonalization]]></title><description><![CDATA[ 
 <br>treat with each problem independent , that give me more space to solve the problem and find easy way (not complex ) to solve the problem <br>Example<br>
optimize cost function (J )<br>
gradient descent <br>Not overfitting<br>
regularization <br>when solve the problem we treat with each other independently   <br>there are some techniques  تكسر هذه القاعدة<br>
<a data-href="Regularization#Early stopping" href="\_zettlenotes\programming-notes\ai-notes\regularization.html#Early_stopping" class="internal-link" target="_self" rel="noopener">Regularization &gt; Early stopping</a><br><br>LINKS TO THIS PAGE <br><br>]]></description><link>_zettlenotes\programming-notes\ai-notes\orthogonalization.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/orthogonalization.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:32:37 GMT</pubDate></item><item><title><![CDATA[Regularization]]></title><description><![CDATA[ 
 <br>Regularization is&nbsp;a set of methods for reducing overfitting in machine learning models.<br><br><br>L2 regularization <br><br>L1 regularization <br><br>w will be sparse <br><br>
<br>الفكرة بشكل عام اني كلما قللت قيمة ال w  كلما كانت ال <a data-href="neural network NN" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a> اصغر و ده الي بيخليها ابسط و بالتالي  بيقلل ال overfitting . 
<br>شرح مفصل اكتر<br>
<img alt="Pasted image 20240729030348.png" src="\lib\media\pasted-image-20240729030348.png" style="width: 500px; max-width: 100%;">
<br><br><br>Dropout regularization is a technique used in machine learning, particularly in neural networks, to prevent overfitting. Overfitting occurs when a model learns the training data too well, including its noise and outliers, leading to poor performance on new, unseen data. Dropout helps to mitigate this by randomly "dropping out" a fraction of the neurons during the training process. Here’s a detailed explanation:<br><br>
<br>
Random Dropping of Neurons: During each training iteration, dropout randomly selects a subset of neurons and sets their output to zero. This means the selected neurons are temporarily removed from the network for that iteration.

<br>
Scaling during Training: To maintain the overall scale of the inputs to the next layer, the remaining neurons are scaled up by a factor of, where ppp is the dropout rate (the probability of dropping a neuron).

<br>
Training Phase vs. Inference Phase:

<br>Training Phase: Dropout is only applied during the training phase. The network becomes an ensemble of many smaller networks, each trained with different subsets of neurons.
<br>Inference Phase: During the inference phase (testing/prediction), dropout is not applied. Instead, the full network is used, but the weights are scaled down by a factor of 1−p1 - p1−p to account for the effect of dropout during training.


<br><img alt="Pasted image 20240729033013.png" src="\lib\media\pasted-image-20240729033013.png" style="width: 200px; max-width: 100%;"><br><br>
<br>Prevents Overfitting: By randomly dropping neurons, dropout forces the network to learn redundant representations and prevents it from relying too heavily on specific neurons.
<br>Improves Generalization: Dropout encourages the network to be more robust and generalize better to new data, as it cannot rely on the presence of any particular neuron during training.
<br><br>
<br>the cost function (J) no longer well defined 
<br><br><br>using some of distortion on the same image or item to increase the size of training data , that reduce the overfitting  .<br>
يعد من المبادئ المستخدمة لحل مشكلة overfitting الا و هو زيادة حجم ال data set <br><br><img alt="Pasted image 20240729040224.png" src="\lib\media\pasted-image-20240729040224.png" style="width: 300px; max-width: 100%;"><br>يعتبر نفس المبدأ بتاع ال l2 regularization الا و هو اختار او تقليل قيمة ال w علي امل ان<br>
ال <a data-href="neural network NN" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a> تكون قيمة ال w مناسبة معاها و لكن ليها عيب واحد الا و هو :<br>
<a data-href="orthogonalization" href="\_zettlenotes\programming-notes\ai-notes\orthogonalization.html" class="internal-link" target="_self" rel="noopener">orthogonalization</a> <br><br>in <a data-href="Regularization" href="\_zettlenotes\programming-notes\ai-notes\regularization.html" class="internal-link" target="_self" rel="noopener">Regularization</a> : we don't delete feature completely , but decrease the effect of  unimportant features <br>in <a data-href="feature selection" href="\_zettlenotes\programming-notes\ai-notes\feature-selection.html" class="internal-link" target="_self" rel="noopener">feature selection</a> : we delete  unimportant features completely <br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/bias and variance.md" data-href="_ZettleNotes/programming Notes/Ai Notes/bias and variance.md" href="\_zettlenotes\programming-notes\ai-notes\bias-and-variance.html" class="internal-link" target="_self" rel="noopener">bias and variance</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/other Notes/cross validation.md" data-href="_ZettleNotes/other Notes/cross validation.md" href="\_zettlenotes\other-notes\cross-validation.html" class="internal-link" target="_self" rel="noopener">cross validation</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/cross validation.md" data-href="_ZettleNotes/programming Notes/Ai Notes/cross validation.md" href="\_zettlenotes\programming-notes\ai-notes\cross-validation.html" class="internal-link" target="_self" rel="noopener">cross validation</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/deep learning specialization.md" data-href="_ZettleNotes/programming Notes/Ai Notes/deep learning specialization.md" href="\_zettlenotes\programming-notes\ai-notes\deep-learning-specialization.html" class="internal-link" target="_self" rel="noopener">deep learning specialization</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/feature selection.md" data-href="_ZettleNotes/programming Notes/Ai Notes/feature selection.md" href="\_zettlenotes\programming-notes\ai-notes\feature-selection.html" class="internal-link" target="_self" rel="noopener">feature selection</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/hyperparameters.md" data-href="_ZettleNotes/programming Notes/Ai Notes/hyperparameters.md" href="\_zettlenotes\programming-notes\ai-notes\hyperparameters.html" class="internal-link" target="_self" rel="noopener">hyperparameters</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/machine learning specialization.md" data-href="_ZettleNotes/programming Notes/Ai Notes/machine learning specialization.md" href="\_zettlenotes\programming-notes\ai-notes\machine-learning-specialization.html" class="internal-link" target="_self" rel="noopener">machine learning specialization</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" data-href="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/orthogonalization.md" data-href="_ZettleNotes/programming Notes/Ai Notes/orthogonalization.md" href="\_zettlenotes\programming-notes\ai-notes\orthogonalization.html" class="internal-link" target="_self" rel="noopener">orthogonalization</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/underverfitting &amp; overfitting.md" data-href="_ZettleNotes/programming Notes/Ai Notes/underverfitting &amp; overfitting.md" href="\_zettlenotes\programming-notes\ai-notes\underverfitting-&amp;-overfitting.html" class="internal-link" target="_self" rel="noopener">underverfitting &amp; overfitting</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Vanishing and Exploding gradients.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Vanishing and Exploding gradients.md" href="\_zettlenotes\programming-notes\ai-notes\vanishing-and-exploding-gradients.html" class="internal-link" target="_self" rel="noopener">Vanishing and Exploding gradients</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\regularization.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/Regularization.md</guid><pubDate>Wed, 31 Jul 2024 20:32:49 GMT</pubDate><enclosure url="lib\media\pasted-image-20240729030348.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240729030348.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Relu Functon]]></title><description><![CDATA[ 
 <br><img alt="Pasted image 20240731023313.png" src="\lib\media\pasted-image-20240731023313.png"><br><br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" data-href="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Vanishing and Exploding gradients.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Vanishing and Exploding gradients.md" href="\_zettlenotes\programming-notes\ai-notes\vanishing-and-exploding-gradients.html" class="internal-link" target="_self" rel="noopener">Vanishing and Exploding gradients</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\relu-functon.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/Relu Functon.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Wed, 31 Jul 2024 20:32:29 GMT</pubDate><enclosure url="lib\media\pasted-image-20240731023313.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240731023313.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[sigmoid function]]></title><description><![CDATA[ 
 <br><img alt="Pasted image 20240731001733.png" src="\lib\media\pasted-image-20240731001733.png" style="width: 500px; max-width: 100%;"><br><br><br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Logistic Regression (Classification ).md" data-href="_ZettleNotes/programming Notes/Ai Notes/Logistic Regression (Classification ).md" href="\_zettlenotes\programming-notes\ai-notes\logistic-regression-(classification-).html" class="internal-link" target="_self" rel="noopener">Logistic Regression (Classification )</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/multilabel label classification.md" data-href="_ZettleNotes/programming Notes/Ai Notes/multilabel label classification.md" href="\_zettlenotes\programming-notes\ai-notes\multilabel-label-classification.html" class="internal-link" target="_self" rel="noopener">multilabel label classification</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" data-href="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\sigmoid-function.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/sigmoid function.md</guid><pubDate>Wed, 31 Jul 2024 20:32:56 GMT</pubDate><enclosure url="lib\media\pasted-image-20240731001733.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240731001733.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Softmax regression]]></title><description><![CDATA[ 
 <br>definition : is generalization of <a data-href="Logistic Regression (Classification )" href="\_zettlenotes\programming-notes\ai-notes\logistic-regression-(classification-).html" class="internal-link" target="_self" rel="noopener">Logistic Regression (Classification )</a> , so able to solve <a data-href="neural network NN#multiclass classification" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html#multiclass_classification" class="internal-link" target="_self" rel="noopener">neural network NN &gt; multiclass classification</a> problem <br><img alt="Pasted image 20240731025428.png" src="\lib\media\pasted-image-20240731025428.png"><br><br><br>يعني الفكرة انت بتشوف نسبة وجود  A من المجموعة (احتمالية و جود A من وسط المجموعة ) . <br>the loss function for softmax in tensorflow : <br>loss = SparseCategoricalCrossEntropy() 
#sparse : take one digit (0 or 1 or 2 ...or 9) 
#CategoricalCrossEntroy : take digit from 0 to 9 
Copy<br><br>decrease Numerical roundoff error <br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Logistic Regression (Classification ).md" data-href="_ZettleNotes/programming Notes/Ai Notes/Logistic Regression (Classification ).md" href="\_zettlenotes\programming-notes\ai-notes\logistic-regression-(classification-).html" class="internal-link" target="_self" rel="noopener">Logistic Regression (Classification )</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/multiclass classification.md" data-href="_ZettleNotes/programming Notes/Ai Notes/multiclass classification.md" href="\_zettlenotes\programming-notes\ai-notes\multiclass-classification.html" class="internal-link" target="_self" rel="noopener">multiclass classification</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/multilabel label classification.md" data-href="_ZettleNotes/programming Notes/Ai Notes/multilabel label classification.md" href="\_zettlenotes\programming-notes\ai-notes\multilabel-label-classification.html" class="internal-link" target="_self" rel="noopener">multilabel label classification</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" data-href="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\softmax-regression.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/Softmax regression.md</guid><pubDate>Wed, 31 Jul 2024 20:33:00 GMT</pubDate><enclosure url="lib\media\pasted-image-20240731025428.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240731025428.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Supervised learning]]></title><description><![CDATA[ 
 <br>definition : we help the machine by provide it the historical date (label) <br><img alt="Pasted image 20240730230715.png#center" src="\lib\media\pasted-image-20240730230715.png" style="width: 500px; max-width: 100%;"><br>any model we must explain some points : <br>
<br>definition of the model algorithm 
<br>the formulation of the model algorithm 
<br>the cost function of the algorithm 
<br>gradient descent   
<br>Topics support supervised learning : <br><a data-href="linear Regression" href="\_zettlenotes\programming-notes\ai-notes\linear-regression.html" class="internal-link" target="_self" rel="noopener">linear Regression</a> - (x-&gt;y) : predict the Number depend on the best line can drawn between points . <br><a data-href="Logistic Regression (Classification )" href="\_zettlenotes\programming-notes\ai-notes\logistic-regression-(classification-).html" class="internal-link" target="_self" rel="noopener">Logistic Regression (Classification )</a> - predict the categories small Number of possible outputs (limited ) . <br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/linear Regression.md" data-href="_ZettleNotes/programming Notes/Ai Notes/linear Regression.md" href="\_zettlenotes\programming-notes\ai-notes\linear-regression.html" class="internal-link" target="_self" rel="noopener">linear Regression</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Logistic Regression (Classification ).md" data-href="_ZettleNotes/programming Notes/Ai Notes/Logistic Regression (Classification ).md" href="\_zettlenotes\programming-notes\ai-notes\logistic-regression-(classification-).html" class="internal-link" target="_self" rel="noopener">Logistic Regression (Classification )</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/machine learning specialization.md" data-href="_ZettleNotes/programming Notes/Ai Notes/machine learning specialization.md" href="\_zettlenotes\programming-notes\ai-notes\machine-learning-specialization.html" class="internal-link" target="_self" rel="noopener">machine learning specialization</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\supervised-learning.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/Supervised learning.md</guid><pubDate>Wed, 31 Jul 2024 20:33:04 GMT</pubDate><enclosure url="lib\media\pasted-image-20240730230715.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240730230715.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[underverfitting & overfitting]]></title><description><![CDATA[ 
 <br><br>the model doesn't fit the training set well , that cause high bias <br>
<img alt="Pasted image 20240731011012.png > right" src="\lib\media\pasted-image-20240731011012.png"><br><br><br>the mode fit the training set extremely well , that cause high variance <br>
<img alt="Pasted image 20240731011048.png > right" src="\lib\media\pasted-image-20240731011048.png"><br><br>
<br>Getting more data ( increase the examples , training data ) 
<br>decreasing the number of features to avoid polynomial  function <a data-href="feature selection" href="\_zettlenotes\programming-notes\ai-notes\feature-selection.html" class="internal-link" target="_self" rel="noopener">feature selection</a>
<br><a data-href="Regularization" href="\_zettlenotes\programming-notes\ai-notes\regularization.html" class="internal-link" target="_self" rel="noopener">Regularization</a> (we don't regularize b parameter  )
<br>to know what is <a data-href="bias and variance" href="\_zettlenotes\programming-notes\ai-notes\bias-and-variance.html" class="internal-link" target="_self" rel="noopener">bias and variance</a><br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/bias and variance.md" data-href="_ZettleNotes/programming Notes/Ai Notes/bias and variance.md" href="\_zettlenotes\programming-notes\ai-notes\bias-and-variance.html" class="internal-link" target="_self" rel="noopener">bias and variance</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/feature selection.md" data-href="_ZettleNotes/programming Notes/Ai Notes/feature selection.md" href="\_zettlenotes\programming-notes\ai-notes\feature-selection.html" class="internal-link" target="_self" rel="noopener">feature selection</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Logistic Regression (Classification ).md" data-href="_ZettleNotes/programming Notes/Ai Notes/Logistic Regression (Classification ).md" href="\_zettlenotes\programming-notes\ai-notes\logistic-regression-(classification-).html" class="internal-link" target="_self" rel="noopener">Logistic Regression (Classification )</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/Regularization.md" data-href="_ZettleNotes/programming Notes/Ai Notes/Regularization.md" href="\_zettlenotes\programming-notes\ai-notes\regularization.html" class="internal-link" target="_self" rel="noopener">Regularization</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\underverfitting-&amp;-overfitting.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/underverfitting &amp; overfitting.md</guid><pubDate>Wed, 31 Jul 2024 20:33:06 GMT</pubDate><enclosure url="lib\media\pasted-image-20240731011012.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pasted-image-20240731011012.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[unsupervised learning]]></title><description><![CDATA[ 
 <br>Find something interesting in unlabeled data  . <br><a data-href="clustering" href="\_zettlenotes\programming-notes\ai-notes\clustering.html" class="internal-link" target="_self" rel="noopener">clustering</a> - Group similar data points together . <br><a data-href="Anomaly detection" href="\Anomaly detection" class="internal-link" target="_self" rel="noopener">Anomaly detection</a> - find unusual data points . <br><a data-href="dimensionality detection" href="\dimensionality detection" class="internal-link" target="_self" rel="noopener">dimensionality detection</a> - compress data using fewer Numbers .<br><br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/clustering.md" data-href="_ZettleNotes/programming Notes/Ai Notes/clustering.md" href="\_zettlenotes\programming-notes\ai-notes\clustering.html" class="internal-link" target="_self" rel="noopener">clustering</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/machine learning specialization.md" data-href="_ZettleNotes/programming Notes/Ai Notes/machine learning specialization.md" href="\_zettlenotes\programming-notes\ai-notes\machine-learning-specialization.html" class="internal-link" target="_self" rel="noopener">machine learning specialization</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\unsupervised-learning.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/unsupervised learning.md</guid><pubDate>Wed, 31 Jul 2024 20:32:07 GMT</pubDate></item><item><title><![CDATA[Vanishing and Exploding gradients]]></title><description><![CDATA[ 
 <br>Vanishing<br>
معناها اني في مرحلة ال <a data-href="Back Propagation" href="\_zettlenotes\programming-notes\ai-notes\back-propagation.html" class="internal-link" target="_self" rel="noopener">Back Propagation</a> ال weights بتبدأ تقل تدريجيا لحد ما توصل لل layers الاولية و بعدين تقترب من الصفر ده بيؤدي انها اكنها مش موجوده يعني ال <a data-href="gradient descent" href="\_zettlenotes\programming-notes\ai-notes\gradient-descent.html" class="internal-link" target="_self" rel="noopener">gradient descent</a> متنفز في ال layers الاولية . <br>Exploding<br>
عكس ال ال vanishing  و هو انك بتبدأ ب ال weights قليله و بعدين بتكبر عادة بشكل exponentially فالبتالي ال layers الاخيرة مش بيحصلها <a data-href="gradient descent" href="\_zettlenotes\programming-notes\ai-notes\gradient-descent.html" class="internal-link" target="_self" rel="noopener">gradient descent</a><br><br><br>initialize weights , use <a data-href="Relu Functon" href="\_zettlenotes\programming-notes\ai-notes\relu-functon.html" class="internal-link" target="_self" rel="noopener">Relu Functon</a> or <a data-href="tansh function" href="\tansh function" class="internal-link" target="_self" rel="noopener">tansh function</a> <br><br><br>check manually on the derivative of the <a data-href="Back Propagation" href="\_zettlenotes\programming-notes\ai-notes\back-propagation.html" class="internal-link" target="_self" rel="noopener">Back Propagation</a> of the <a data-href="neural network NN" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a> to avoid any type of error when run the deep Neural network . <br><br><br>
<br>Don't use gradient checking in training - only to debug 
<br>if algorithm fails grad check , look at components to try to identify bug . 
<br>remember <a data-href="Regularization" href="\_zettlenotes\programming-notes\ai-notes\regularization.html" class="internal-link" target="_self" rel="noopener">Regularization</a>
<br>doesn't work with dropout 
<br>run at random initialization ; perhaps again after some training 
<br><br>LINKS TO THIS PAGE <br><br><br><br><br><br>]]></description><link>_zettlenotes\programming-notes\ai-notes\vanishing-and-exploding-gradients.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/Vanishing and Exploding gradients.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Mon, 05 Aug 2024 12:30:13 GMT</pubDate></item><item><title><![CDATA[vectorization]]></title><description><![CDATA[ 
 <br>refers to the process of converting data into a numerical format that a machine learning model can process. It involves transforming data into vectors, which are arrays of numbers. This is crucial for efficiently performing mathematical operations and leveraging optimized linear algebra libraries. Here are a few key aspects of vectorization<br>LINKS TO THIS PAGE <br><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/deep learning specialization.md" data-href="_ZettleNotes/programming Notes/Ai Notes/deep learning specialization.md" href="\_zettlenotes\programming-notes\ai-notes\deep-learning-specialization.html" class="internal-link" target="_self" rel="noopener">deep learning specialization</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/linear Regression.md" data-href="_ZettleNotes/programming Notes/Ai Notes/linear Regression.md" href="\_zettlenotes\programming-notes\ai-notes\linear-regression.html" class="internal-link" target="_self" rel="noopener">linear Regression</a><br><a data-tooltip-position="top" aria-label="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" data-href="_ZettleNotes/programming Notes/Ai Notes/neural network NN.md" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a>]]></description><link>_zettlenotes\programming-notes\ai-notes\vectorization.html</link><guid isPermaLink="false">_ZettleNotes/programming Notes/Ai Notes/vectorization.md</guid><pubDate>Wed, 31 Jul 2024 20:32:02 GMT</pubDate></item><item><title><![CDATA[Binary Cross-Entropy Loss]]></title><description><![CDATA[ 
 ]]></description><link>_zettlenotes\binary-cross-entropy-loss.html</link><guid isPermaLink="false">_ZettleNotes/Binary Cross-Entropy Loss.md</guid><pubDate>Thu, 01 Aug 2024 16:43:06 GMT</pubDate></item><item><title><![CDATA[Test note]]></title><description><![CDATA[ 
 <br>how to create  Mathematical Notes in obsidian <br><br><br>this is very easy <br><br>LINKS TO THIS PAGE <br>Dataview: No results to show for list query.]]></description><link>_zettlenotes\test-note.html</link><guid isPermaLink="false">_ZettleNotes/Test note.md</guid><dc:creator><![CDATA[Andrew ng]]></dc:creator><pubDate>Mon, 05 Aug 2024 00:34:29 GMT</pubDate></item><item><title><![CDATA[Drawing 2024-07-26 02.24.38]]></title><description><![CDATA[ 
 
  
  
  
    
    
  
  العقيدة درس العقيدة غرس العقيدة معلومات نظرية التأمل التفكر شعورك بحقيقة نفسك و حقيقة الله عز و جل ]]></description><link>excalidraw\drawing-2024-07-26-02.24.38.excalidraw.html</link><guid isPermaLink="false">Excalidraw/Drawing 2024-07-26 02.24.38.excalidraw.md</guid><pubDate>Thu, 25 Jul 2024 23:30:12 GMT</pubDate></item><item><title><![CDATA[Drawing 2024-07-31 01.17.25]]></title><description><![CDATA[ 
 
  
  
  
    
    
  
  ]]></description><link>excalidraw\drawing-2024-07-31-01.17.25.excalidraw.html</link><guid isPermaLink="false">Excalidraw/Drawing 2024-07-31 01.17.25.excalidraw.md</guid><pubDate>Tue, 30 Jul 2024 22:17:25 GMT</pubDate></item><item><title><![CDATA[Drawing 2024-07-31 01.58.21]]></title><description><![CDATA[ 
 
  
  
  
    
    
  
  inputs aoutputs ]]></description><link>excalidraw\drawing-2024-07-31-01.58.21.excalidraw.html</link><guid isPermaLink="false">Excalidraw/Drawing 2024-07-31 01.58.21.excalidraw.md</guid><pubDate>Tue, 30 Jul 2024 23:00:13 GMT</pubDate></item><item><title><![CDATA[Drawing 2024-07-31 03.05.17]]></title><description><![CDATA[ 
 
  
  
  
    
    
  
  ]]></description><link>excalidraw\drawing-2024-07-31-03.05.17.excalidraw.html</link><guid isPermaLink="false">Excalidraw/Drawing 2024-07-31 03.05.17.excalidraw.md</guid><pubDate>Wed, 31 Jul 2024 00:07:25 GMT</pubDate></item><item><title><![CDATA[شكل الصفحة]]></title><description><![CDATA[ 
 
  
  
  
    
    
  
  التاريخمكان الكتابةمكان التعليقات و الاسئلة و الاضافات العنوان]]></description><link>excalidraw\شكل-الصفحة.html</link><guid isPermaLink="false">Excalidraw/شكل الصفحة.md</guid><pubDate>Tue, 23 Jul 2024 18:18:50 GMT</pubDate></item><item><title><![CDATA[General Notes]]></title><description><![CDATA[ 
 <br><br>LINKS TO THIS PAGE <br>Dataview: No results to show for list query.]]></description><link>templates\general-notes.html</link><guid isPermaLink="false">Templates/General Notes.md</guid><pubDate>Mon, 05 Aug 2024 12:21:15 GMT</pubDate></item><item><title><![CDATA[index]]></title><description><![CDATA[ 
 <br>the important link<br>
<a data-href="neural network NN" href="\_zettlenotes\programming-notes\ai-notes\neural-network-nn.html" class="internal-link" target="_self" rel="noopener">neural network NN</a>]]></description><link>index.html</link><guid isPermaLink="false">index.md</guid><pubDate>Mon, 05 Aug 2024 13:33:51 GMT</pubDate></item></channel></rss>