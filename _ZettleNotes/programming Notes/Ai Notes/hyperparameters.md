---
date: 2024-07-30
author:
  - Andrew ng
topic:
  - Ai
  - machine learning
---
### what is the  different between hyper tuning (hyperparameter) and gradient descent ? 

**hyperparameters**
optimization of hyper parameters like (learning rate , batch size  , N of layers , N of neurons ,  [Regularization](Regularization.md) parameters )
focuses on finding the best hyperparameters that control the training process and model to achieve the best performance 

**gradient descent**
optimization of model parameters 
focuses on adjusting the model parameters to minimize the loss function during training . 









----
LINKS TO THIS PAGE 
```dataview
LIST FROM ([#](#)) OR outgoing([#](#)) WHERE file.name != this.file.name SORT file.name ASC 
```
